\chapter{Rappels et compléments d'algèbre linéaire}

\minitoc

Dans tout ce chapitre, \(\K\) désigne un sous-corps de \(\C\), en général \(\R\) ou \(\C\).

\section{Sommes de sous-espaces vectoriels}

\subsection{Généralités}

\begin{defi}
Soient \(E\) un \(\K\)-espace vectoriel et \(F_1,\dots,F_n\) des sous-espaces vectoriels de \(E\).

On appelle somme de \(F_1,\dots,F_n\) l'ensemble noté \(F_1+\dots+F_n\) : \[F_1+\dots+F_n=\accol{\sum_{i=1}^nx_i\tq\paren{x_1,\dots,x_n}\in F_1\times\dots\times F_n}.\]
\end{defi}

\begin{prop}
Soit \(\paren{u_1,\dots,u_p,u_{p+1},\dots,u_n}\in E^n\).

Alors \(\Vect{u_1,\dots,u_p,u_{p+1},\dots,u_n}=\Vect{u_1,\dots,u_p}+\Vect{u_{p+1},\dots,u_n}\).
\end{prop}

\begin{prop}
Avec les mêmes notations : \(F_1+\dots+F_n\) est un sous-espace vectoriel de \(E\).

De plus, c'est le plus petit sous-espace vectoriel qui contient \(F_1,\dots,F_n\).

Si on connaît des familles génératrices de chacun des sous-espaces vectoriels \(F_1,\dots,F_n\), alors en concaténant ces familles, on obtient une famille génératrice de \(F_1+\dots+F_n\).
\end{prop}

Conséquence : en fractionnant une famille génératrice de \(E\) en sous-familles, on décompose l'espace \(E\) en une somme de sous-espaces vectoriels.

\subsection{Sommes directes}

\begin{defi}
Soient \(E\) un \(\K\)-espace vectoriel et \(F_1,\dots,F_n\) des sous-espaces vectoriels de \(E\).

On dit que la somme \(F_1+\dots+F_n\) est directe quand tout vecteur de \(F_1+\dots+F_n\) a une unique écriture \(\sum_{i=1}^nx_i\) où \(\paren{x_1,\dots,x_n}\in F_1\times\dots\times F_n\).
\end{defi}

On dit aussi que les sous-espaces sont en somme directe. Dans ce cas, quand on veut insister sur cette propriété, on note la somme sous la forme \(F_1\oplus\dots\oplus F_n=\bigoplus_{i=1}^nF_i\).

\begin{prop}
Avec les mêmes hypothèses.

La somme \(F_1+\dots+F_n\) est directe ssi le vecteur nul a une unique décomposition \(\sum_{i=1}^nx_i\) où \(\paren{x_1,\dots,x_n}\in F_1\times\dots\times F_n\), qui est la décomposition triviale.

Autrement dit, la somme \(F_1+\dots+F_n\) est directe ssi la seule solution de l'équation \(\sum_{i=1}^nx_i=0\) d'inconnue \(\paren{x_1,\dots,x_n}\in F_1\times\dots\times F_n\) est le \(n\)-uplet nul.
\end{prop}

\begin{dem}
\impdir

Immédiat : le vecteur nul appartient à \(F_1+\dots+F_n\) et a une unique écriture sous la forme \(\sum_{i=1}^nx_i\) où \(\paren{x_1,\dots,x_n}\in F_1\times\dots\times F_n\).

Or on en connaît une : \(0=0+\dots+0\).

Donc la seule solution à l'équation \(\sum_{i=1}^nx_i=0\) d'inconnue \(\paren{x_1,\dots,x_n}\in F_1\times\dots\times F_n\) est la solution triviale \(\paren{0,\dots,0}\).

\imprec

Soit \(z\in F_1+\dots+F_n\).

On veut montrer que \(z\) a une unique écriture \(z=\sum_{i=1}^nx_i\) où \(\paren{x_1,\dots,x_n}\in F_1\times\dots\times F_n\).

Si \(z=\sum_{i=1}^nx_i=\sum_{i=1}^nx_i\prim\) où \(\paren{x_1,\dots,x_n},\paren{x_1\prim,\dots,x_n\prim}\in F_1\times\dots\times F_n\), alors \(\sum_{i=1}^n\paren{x_i-x_i\prim}=0\).

Or \(\quantifs{\tpt i\in\interventierii{1}{n}}x_i-x_i\prim\in F_i\) car \(F_i\) est un sous-espace vectoriel.

Ainsi, \(\quantifs{\tpt i\in\interventierii{1}{n}}x_i-x_i\prim=0\) \ie \(\paren{x_1,\dots,x_n}=\paren{x_1\prim,\dots,x_n\prim}\).

D'où l'unicité voulue.
\end{dem}

Un exemple fondamental : si \(\paren{v_1,\dots,v_n}\) est une famille libre, alors les droites vectorielles \(\Vect{v_i}\) sont en somme directe.

\begin{prop}
Avec les mêmes hypothèses.

Si la somme \(F_1+\dots+F_n\) est directe, alors en concaténant des familles libres de chacun des sous-espaces vectoriels, on obtient une famille libre.
\end{prop}

\begin{dem}
Si \(F_1+\dots+F_n\) est une somme directe alors soient \(L_1,\dots,L_n\) des familles libres de \(F_1,\dots,F_n\) respectivement.

On note \(L_i=\paren{e_{i\,1},\dots,e_{i\,l_i}}\) où \(i\in\interventierii{1}{n}\).

Soit \(\paren{\lambda_{i\,j}}_{\substack{1\leq i\leq n \\ 1\leq j\leq l_i}}\) une famille de scalaires telle que \(\sum_{\substack{1\leq i\leq n \\ 1\leq j\leq l_i}}\lambda_{i\,j}e_{i\,j}=0\).

Alors \(\sum_{i=1}^n\sum_{j=1}^{l_i}\lambda_{i\,j}e_{i\,j}=0\).

Or \(F_1+\dots+F_n\) est directe donc \(\quantifs{\tpt i\in\interventierii{1}{n}}\sum_{j=1}^{l_i}\lambda_{i\,j}e_{i\,j}=0\).

Or \(L_i\) est libre donc \(\lambda_{i\,1}=\dots=\lambda_{i\,l_i}=0\).

Donc la concaténation de \(L_1,\dots,L_n\) est libre.
\end{dem}

\subsection{Sous-espaces supplémentaires}

\begin{defi}
Soient \(E\) un \(\K\)-espace vectoriel et \(F_1,\dots,F_n\) des sous-espaces vectoriels de \(E\).

On dit que les sous-espaces \(F_1,\dots,F_n\) sont supplémentaires (dans \(E\)) quand \(E=\bigoplus_{i=1}^nF_i\).
\end{defi}

On déduit des deux parties précédentes le résultat de la décomposition d'un vecteur.

\begin{prop}
Soient \(E\) un \(\K\)-espace vectoriel et \(F_1,\dots,F_n\) des sous-espaces vectoriels de \(E\).

Il y a équivalence entre :

\begin{itemize}
    \item les sous-espaces \(F_1,\dots,F_n\) sont supplémentaires \\
    \item tout vecteur de \(E\) peut s'écrire de façon unique comme somme de vecteurs des sous-espaces vectoriels \(F_1,\dots,F_n\) : \[\quantifs{\forall v\in E;\exists!\paren{v_1,\dots,v_n}\in F_1\times\dots\times F_n}v=\sum_{i=1}^nv_i.\]
\end{itemize}
\end{prop}

Dans ce cas, soit \(v\) un vecteur de \(E\). Il existe un unique \(n\)-uplet \(\paren{v_1,\dots,v_n}\in F_1\times\dots\times F_n\) tel que \(v=\sum_{i=1}^nv_i\).

Pour tout \(j\in\interventierii{1}{n}\), on définit \(p_j:E\to E\) en posant \(p_j\paren{v}=v_j\).

Alors les applications \(p_j\) sont des projecteurs qui vérifient les propriétés :

\begin{itemize}
    \item \(\sum_{i=1}^np_i=\id{E}\) \\
    \item \(\quantifs{\tpt\paren{i,j}\in\interventierii{1}{n}^2}p_i\rond p_j=\delta_{i\,j}p_i\) où \(\delta_{x\,y}\) est le symbole de Kronecker.
\end{itemize}

La réciproque est vraie : si \(\paren{p_1,\dots,p_n}\) sont \(n\) projecteurs vérifiant les deux propriétés précédentes, alors les sous-espaces \(\paren{\Im p_i}_{1\leq i\leq n}\) sont supplémentaires.

\begin{dem}
Soient \(F_1,\dots,F_n\) supplémentaires dans \(E\).

\begin{itemize}
    \item \(\quantifs{\Tpt j\in\interventierii{1}{n}}p_j\) est un projecteur : \\\\ Soit \(v\in E\). On écrit \(v=v_1+\dots+v_j+\dots+v_n\). \\\\ Par définition, on a \(p_j\paren{v}=v_j=0+\dots+0+v_j+0+\dots+0\). \\\\ Donc, par définition, on a \(p_j\paren{p_j\paren{v}}=p_j\paren{v_j}=v_j\). \\\\ Donc \(p_j^2=p_j\). \\\\ Donc \(p\) est un projecteur (la linéarité est évidente). \\
    \item \(\quantifs{\Tpt v\in E}v=v_1+\dots+v_n=p_1\paren{v}+\dots+p_n\paren{v}\) donc \(\sum_{i=1}^np_i=\id{E}\). \\
    \item Montrons que \(\quantifs{\tpt\paren{i,j}\in\interventierii{1}{n}^2}p_ip_j=\delta_{i\,j}p_i\). \\\\ Si \(i=j\), c'est vrai car \(p_i\) est un projecteur. \\\\ Si \(i\not=j\), soit \(v\in E\). On écrit \(v=v_1+\dots+v_n\). \\\\ Par définition, \(p_j\paren{v}=v_j=0+\dots+0+v_j+0+\dots+0\). \\\\ D'où \(p_ip_j\paren{v}=0\).
\end{itemize}

Réciproquement, montrons que si \(p_1,\dots,p_n\) sont des projecteurs tels que \(\sum_{i=1}^np_i=\id{E}\) et \(\quantifs{\forall\paren{i,j}\in\interventierii{1}{n}^2}p_ip_j=\delta_{i\,j}p_i\), alors \(E=\bigoplus_{i=1}^n\Im p_i\).

\begin{itemize}
    \item Montrons que \(\Im p_1+\dots+\Im p_n\) est directe. \\\\ Soit \(\paren{v_1,\dots,v_n}\in\Im p_1\times\dots\times\Im p_n\) telle que \(v_1+\dots+v_n=0\). \\\\ Pour \(i\in\interventierii{1}{n}\), on a \[\begin{aligned}
        p_i\paren{0}=0&=p_i\paren{v_1}+\dots+p_i\paren{v_n} \\
        &=p_ip_1\paren{v_1}+\dots+p_ip_i\paren{v_i}+\dots+p_ip_n\paren{v_n} \\
        &=v_i. \\
    \end{aligned}\] Donc \(v_1=\dots=v_n=0\). \\\\ Donc la somme est directe. \\
    \item On a \(\sum_{i=1}^np_i=\id{E}\) donc \(\quantifs{\tpt v\in E}v=\sum_{i=1}^np_i\paren{v}\). \\\\ Donc \(v\in\Im p_1+\dots+\Im p_n\).
\end{itemize}

Finalement, \(E=\bigoplus_{i=1}^n\Im p_i\).
\end{dem}

\begin{exo}
Soit \(E=\F{\C}{\C}\). Pour \(k\in\accol{0,1,2}\), on pose \(E_k=\accol{f\in E\tq\quantifs{\forall x\in\C}f\paren{jx}=j^kf\paren{x}}\).

Montrez que \(E_0,E_1,E_2\) sont trois sous-espaces vectoriels de \(E\) supplémentaires.
\end{exo}

\begin{corr}
\begin{itemize}
    \item Soit \(k\in\accol{0,1,2}\). \(E_k\) contient l'application nulle. \\\\ Soient \(\paren{f,g}\in E_k^2\), \(\lambda\in\C\) et \(x\in\C\). \\\\ On a \[\begin{aligned}
        \paren{\lambda f+g}\paren{jx}&=\lambda f\paren{jx}+g\paren{jx} \\
        &=\lambda j^kf\paren{x}+j^kg\paren{x} \\
        &=j^k\paren{\lambda f\paren{x}+g\paren{x}} \\
        &=j^k\paren{\lambda f+g}\paren{x}. \\
    \end{aligned}\] Donc \(\lambda f+g\in E_k\) \ie \(E_k\) est un sous-espace vectoriel de \(E\). \\
    \item Montrons que \(\quantifs{\forall f\in E;\exists!\paren{f_0,f_1,f_2}\in E_0\times E_1\times E_2}f=f_0+f_1+f_2\). \\\\ \analyse \\\\ Soient \(f\in E\) et \(\paren{f_0,f_1,f_2}\in E_0\times E_1\times E_2\) telles que \(f=f_0+f_1+f_2\). \\\\ Pour tout \(x\in\C\), on a \[f\paren{jx}=f_0\paren{jx}+f_1\paren{jx}+f_2\paren{jx}=f_0\paren{x}+jf_1\paren{x}+j^2f_2\paren{x}\] et \[f\paren{j^2x}=f_0\paren{x}+j^2f_1\paren{x}+jf_2\paren{x}.\] D'où \[\begin{dcases}
        f_0\paren{x}+f_1\paren{x}+f_2\paren{x}=f\paren{x} \\
        f_0\paren{x}+jf_1\paren{x}+j^2f_2\paren{x}=f\paren{jx} \\
        f_0\paren{x}+j^2f_1\paren{x}+jf_2\paren{x}=f\paren{j^2x}
    \end{dcases}\] Or \(\begin{vmatrix}
        1 & 1 & 1 \\
        1 & j & j^2 \\
        1 & j^2 & j^4
    \end{vmatrix}\) est un déterminant de Vandermonde et est donc non-nul. \\\\ En faisant \(L_1+L_2+L_3\), on obtient \[f_0\paren{x}=\dfrac{1}{3}\paren{f\paren{x}+f\paren{jx}+f\paren{j^2x}}.\] En faisant \(L_1+j^2L_2+jL_3\), on obtient \[f_1\paren{x}=\dfrac{1}{3}\paren{f\paren{x}+j^2f\paren{jx}+jf\paren{j^2x}}.\] En faisant \(L_1+jL_2+j^2L_3\), on obtient \[f_2\paren{x}=\dfrac{1}{3}\paren{f\paren{x}+jf\paren{jx}+j^2f\paren{j^2x}}.\] Ceci prouve l'unicité de la décomposition (si elle existe). \\\\ \synthese \\\\ On pose \(f_k:x\mapsto\dfrac{1}{3}\paren{f\paren{x}+j^{2k}f\paren{jx}+j^kf\paren{j^2x}}\) pour \(k\in\accol{0,1,2}\). \\\\ On a \(\quantifs{\forall x\in\C}f\paren{x}=f_0\paren{x}+f_1\paren{x}+f_2\paren{x}\) \ie \(f=f_0+f_1+f_2\). \\\\ De plus, on a \(f_0\paren{jx}=\dfrac{1}{3}\paren{f\paren{jx}+f\paren{j^2x}+f\paren{j^3x}}=f_0\paren{x}\) car \(j^3=1\) donc \(f_0\in E_0\). \\\\ On montre de même que \(f_1\in E_1\) et \(f_2\in E_2\). \\\\ \conclusion On a bien \(E=\bigoplus_{k=0}^2E_k\).
\end{itemize}
\end{corr}

\begin{prop}
Avec les mêmes hypothèses.

Si les sous-espaces \(F_1,\dots,F_n\) sont supplémentaires, alors en concaténant des bases de chacun des sous-espaces vectoriels, on obtient une base de \(E\).
\end{prop}

\subsection{Cas particulier de deux sous-espaces}

\begin{prop}
Soient \(E\) un \(\K\)-espace vectoriel et \(F,G\) deux sous-espaces vectoriels de \(E\).

La somme \(F+G\) est directe ssi \(F\inter G=\accol{0}\).
\end{prop}

Attention ! Il ne faut pas généraliser à trois ou plus sous-espaces. Même si \(F_1\inter F_2\inter F_3\) est le sous-espace nul, on ne peut pas conclure que la somme \(F_1+F_2+F_3\) est directe.

\subsection{Applications linéaires et sommes directes}

\begin{prop}
Soient \(E,F\) deux \(\K\)-espaces vectoriels, \(E_1,\dots,E_n\) des sous-espaces vectoriels supplémentaires dans \(E\) et \(f_1,\dots,f_n\) des applications linéaires de \(E_1,\dots,E_n\) dans \(F\) respectivement.

Alors il existe une unique application linéaire \(f\) de \(E\) dans \(F\) telle que \(\quantifs{\tpt i\in\interventierii{1}{n}}\restr{f}{E_i}=f_i\).
\end{prop}

Autrement dit, pour définir une application linéaire sur une somme directe de sous-espaces vectoriels, il suffit de la définir sur chacun des sous-espaces vectoriels.

\begin{dem}
\analyse

On suppose que \(f\) existe telle que \(\quantifs{\forall i\in\interventierii{1}{n}}\restr{f}{E_i}=f_i\).

Soit \(x\in E\).

Comme \(E=\bigoplus_{i=1}^nE_i\), il existe un unique \(n\)-uplet \(\paren{x_1,\dots,x_n}\in E_1\times\dots\times E_n\) tel que \(x=\sum_{i=1}^nx_i\).

\(f\) étant linéaire, on a \(f\paren{x}=\sum_{i=1}^nf\paren{x_i}\).

Or \(\quantifs{\tpt i\in\interventierii{1}{n}}x_i\in E_i\) et \(\restr{f}{E_i}=f_i\) donc \(f\paren{x_i}=f_i\paren{x_i}\).

Donc \(f\paren{x}=\sum_{i=1}^nf_i\paren{x_i}\).

L'analyse prouve l'unicité de \(f\) (si elle existe).

\synthese

Pour \(x\in E\) qu'on écrit \(\sum_{i=1}^nx_i\) où \(\paren{x_1,\dots,x_n}\in E_1\times\dots\times E_n\), on pose \(f\paren{x}=\sum_{i=1}^nf_i\paren{x_i}\).

\begin{itemize}
    \item Montrons que \(\quantifs{\tpt i\in\interventierii{1}{n}}\restr{f}{E_i}=f_i\). \\\\ Pour \(i\in\interventierii{1}{n}\), pour \(x\in E_i\), on a \(x=0+\dots+0+x+0+\dots+0\). \\\\ Donc \(f\paren{x}=f_1\paren{0}+\dots+f_{i-1}\paren{0}+f_i\paren{x}+f_{i+1}\paren{0}+\dots+f_n\paren{0}=f_i\paren{x}\). \\\\ Donc \(\restr{f}{E_i}=f_i\). \\
    \item Montrons que \(f\) est linéaire. \\\\ Soient \(\paren{x,y}\in E^2\) et \(\lambda\in\K\). \\\\ On écrit \(x=\sum_{i=1}^nx_i\) et \(y=\sum_{i=1}^ny_i\) où \(\paren{x_1,\dots,x_n},\paren{y_1,\dots,y_n}\in E_1\times\dots\times E_n\). \\\\ Alors \[\begin{aligned}
        f\paren{\lambda x+y}&=\sum_{i=1}^nf_i\paren{\lambda x_i+y_i} \\
        &=\sum_{i=1}^n\paren{\lambda f_i\paren{x_i}+f_i\paren{y_i}} \\
        &=\lambda\sum_{i=1}^nf_i\paren{x_i}+\sum_{i=1}^nf_i\paren{y_i} \\
        &=\lambda f\paren{x}+f\paren{y}.
    \end{aligned}\]
\end{itemize}
\end{dem}

\begin{exo}
Soient \(E_1,\dots,E_n\) des sous-espaces vectoriels supplémentaires. Quelles sont les applications qui induisent l'application identité sur un des \(E_i\) et l'application nulle sur les autres \(E_j\) ?
\end{exo}

\begin{corr}
On pose \(f=\id{E_i}\) sur \(E_i\) et \(f=0\) sur \(E_j\) pour \(j\not=i\).

\(f\) est le projecteur sur \(E_i\) parallèlement à \(\bigoplus_{j\not=i}E_j\).
\end{corr}

\section{Somme de sous-espaces vectoriels en dimension finie}

\subsection{Base adaptée à un sous-espace}

\begin{defi}
Soient \(E\) un \(\K\)-espace vectoriel de dimension \(n\) et \(F\) un sous-espace vectoriel de \(E\) de dimension \(p\).

On appelle base de \(E\) adaptée à \(F\) toute base de \(E\) qui contient une base de \(F\). Quitte à changer l'ordre des vecteurs, on peut suppose dans une base adaptée à \(F\) que les \(p\) premiers vecteurs de la base forment une base de \(F\).
\end{defi}

\begin{defi}
Soient \(E\) un \(\K\)-espace vectoriel de dimension \(n\) et \(\paren{F_1,\dots,F_p}\) une famille de sous-espaces vectoriels supplémentaires dans \(E\).

On appelle base adaptée à la somme \(E=\bigoplus_{i=1}^pF_i\) la concaténation de bases de chacun des sous-espaces \(F_1,\dots,F_p\) (dans cet ordre).
\end{defi}

Si \(\fami{B}\) est une base de \(E\), alors en fractionnant la base en sous-familles, les sous-espaces vectoriels engendrés par chacune de ces sous-familles sont supplémentaires et la base est alors adaptée à la somme des sous-espaces.

\subsection{Sommes directes et bases}

On donne un moyen simple de vérifier qu'une somme est directe, voire plus.

\begin{prop}
Soient \(E\) un \(\K\)-espace vectoriel de dimension \(n\) et \(\paren{F_1,\dots,F_p}\) une famille de sous-espaces vectoriels de \(E\).

Si en concaténant des bases de chacun des sous-espaces vectoriels \(F_1,\dots,F_p\) on obtient une famille libre, alors les sous-espaces vectoriels sont en somme directe.

Si en concaténant des bases de chacun des sous-espaces vectoriels \(F_1,\dots,F_p\) on obtient une base de \(E\), alors les sous-espaces vectoriels sont supplémentaires.
\end{prop}

\subsection{Dimension d'une somme de sous-espaces vectoriels}

\begin{prop}
Soient \(E\) un \(\K\)-espace vectoriel de dimension finie et \(F_1,\dots,F_n\) des sous-espaces vectoriels de \(E\).

Alors \(\dim\sum_{i=1}^nF_i\leq\sum_{i=1}^n\dim F_i\).
\end{prop}

\begin{dem}
On note \(p_i=\dim F_i\) pour \(i\in\interventierii{1}{n}\).

Pour tout \(i\in\interventierii{1}{n}\), on choisit une base \(\fami{B}_i\) de \(F_i\) et on a \(F_i=\Vect{\fami{B}_i}\).

Donc \(F_1+\dots+F_n=\Vect{\fami{B}_1\dots\fami{B}_n}\).

Donc \[\begin{aligned}
\dim\paren{F_1+\dots+F_n}&\leq\Card\paren{\fami{B}_1\dots\fami{B}_n} \\
&=\sum_{i=1}^n\dim F_i.
\end{aligned}\]
\end{dem}

\begin{dem}[Autre méthode]\thlabel{dem4.10}
On pose l'application linéaire \[\fonction{\phi}{F_1\times\dots\times F_n}{E}{\paren{x_1,\dots,x_n}}{\sum_{i=1}^nx_i}\]

On a \(\Im\phi=F_1+\dots+F_n\).

Donc \[\begin{aligned}
\dim\paren{F_1+\dots+F_n}&=\dim\Im\phi \\
&=\rg\phi \\
&=\dim\paren{F_1\times\dots\times F_n}-\dim\ker\phi \\
&\leq\dim\paren{F_1\times\dots\times F_n} \\
&=\sum_{i=1}^n\dim F_i.
\end{aligned}\]
\end{dem}

Il y a égalité quand la somme est directe.

\begin{theo}
Soient \(E\) un \(\K\)-espace vectoriel de dimension finie et \(F_1,\dots,F_n\) des sous-espaces vectoriels de \(E\).

Alors \(F_1,\dots,F_n\) sont en somme directe ssi \(\dim\sum_{i=1}^nF_i=\sum_{i=1}^n\dim F_i\).
\end{theo}

\begin{dem}
On reprend la \thref{dem4.10}.

Si \(\dim\sum_{i=1}^nF_i=\sum_{i=1}^n\dim F_i\), alors \(\dim\ker\phi=0\).

Donc \(\ker\phi=\accol{\paren{x_1,\dots,x_n}\in F_1\times\dots\times F_n\tq\sum_{i=1}^nx_i=0}=\accol{\paren{0,\dots,0}}\).

Ainsi, \(F_1+\dots+F_n\) est directe.

Et réciproquement.
\end{dem}

\subsection{Sous-espaces supplémentaires}

En dimension finie, on a une façon plus simple de prouver que des sous-espaces vectoriels sont supplémentaires.

\begin{prop}[Trois pour le prix de deux]
Soient \(E\) un \(\K\)-espace vectoriel de dimension finie et \(F_1,\dots,F_n\) des sous-espaces vectoriels de \(E\).

Quand deux propriétés parmi les trois suivantes sont vraies, alors la troisième l'est aussi :

\begin{itemize}
    \item \(E=\sum_{i=1}^nF_i\) \\
    \item \(\dim\sum_{i=1}^nF_i=\sum_{i=1}^n\dim F_i\) \\
    \item \(\dim E=\sum_{i=1}^n\dim F_i\)
\end{itemize}

Donc dans ce cas, les sous-espaces vectoriels \(F_1,\dots,F_n\) sont supplémentaires.
\end{prop}

En pratique, le cas le plus utile est le suivant : si \(\dim\sum_{i=1}^nF_i=\sum_{i=1}^n\dim F_i=\dim E\), alors les sous-espaces vectoriels \(F_1,\dots,F_n\) sont supplémentaires.

\begin{dem}~\\
Si \(\dim\sum_{i=1}^nF_i=\sum_{i=1}^n\dim F_i\) et \(\dim\sum_{i=1}^nF_i=\dim E\).

Alors \(F_1+\dots+F_n\) est directe et \(\sum_{i=1}^nF_i=E\).

Alors \(E=\bigoplus_{i=1}^nF_i\).
\end{dem}

\begin{exo}
Dans \(\R^4\), soient \(H=\accol{\paren{x,y,z,t}\in\R^4\tq x+z+t=0\text{ et }x-y-z+t=0}\), \(F=\Vect{\paren{-1,0,1,1}}\) et \(G=\Vect{\paren{2,-1,1,0}}\).

\(F\), \(G\) et \(H\) sont-ils supplémentaires ?
\end{exo}

\begin{corr}
\(H\) est l'ensemble des solutions d'un système linéaire homogène : \[S:\begin{dcases}
x+z+t=0 \\
x-y-z+t=0
\end{dcases}\] \ie \(H=\ker f\) où \(f:\paren{x,y,z,t}\mapsto\paren{x+z+t,x-y-z+t}\).

D'après le théorème du rang, on a \(\dim H=\dim\R^4-\rg S=4-2=2\).

\(F\) et \(G\) sont des droites vectorielles donc \(\dim F=\dim G=1\).

On a donc \(\dim\R^4=\dim F+\dim G+\dim H\).

On a \[\begin{aligned}
v=\paren{x,y,z,t}\in H&\ssi\begin{dcases}
x+z+t=0 \\
x-y-z+t=0
\end{dcases} \\
&\ssi\begin{dcases}
x=-z-t \\
y=-2z
\end{dcases} \\
&\ssi v=\paren{-z-t,-2z,z,t} \\
&\ssi v=z\underbrace{\paren{-1,-2,1,0}}_{h_1}+t\underbrace{\paren{-1,0,0,1}}_{h_2}.
\end{aligned}\]

Donc \(H=\Vect{h_1,h_2}\).

On note \(f=\paren{-1,0,1,1}\) et \(g=\paren{2,-1,1,0}\).

On a alors \(F+G+H=\Vect{f,g,h_1,h_2}\).

Donc \(\dim\paren{F+G+H}=\rg\paren{f,g,h_1,h_2}\).

On note \(\fami{B}_0\) la base canonique de \(\R^4\).

On a \[\begin{aligned}
A=\Mat[\fami{B}_0]{f,g,h_1,h_2}&=\begin{pmatrix}
-1 & 2 & -1 & -1 \\
0 & -1 & -2 & 0 \\
1 & 1 & 1 & 0 \\
1 & 0 & 0 & 1
\end{pmatrix} \\
&\simqd{\substack{L_3\gets L_3+L_1 \\ L_4\gets L_4+L_1}}\begin{pmatrix}
-1 & 2 & -1 & -1 \\
0 & -1 & -2 & 0 \\
0 & 3 & 0 & -1 \\
0 & 2 & -1 & 0
\end{pmatrix} \\
&\simqd{\substack{L_3\gets L_3+3L_2 \\ L_4\gets L_4+2L_2}}\begin{pmatrix}
-1 & 2 & -1 & -1 \\
0 & 1 & -2 & 0 \\
0 & 0 & -6 & -1 \\
0 & 0 & -5 & 0
\end{pmatrix}.
\end{aligned}\]

Donc \(\rg A=4\).

Donc \(\dim\paren{F+G+H}=\dim\R^4=\dim F+\dim G+\dim H\).

Finalement, on a \(\R^4=F\oplus G\oplus H\).
\end{corr}

\subsection{Dimension d'une somme de deux sous-espaces vectoriels}

Rappel : la formule de Grassmann.

\begin{prop}
Soient \(E\) un \(\K\)-espace vectoriel de dimension finie et \(F,G\) deux sous-espaces vectoriels de \(E\).

Alors \(\dim\paren{F+G}=\dim F+\dim G-\dim\paren{F\inter G}\).
\end{prop}

\section{Polynômes d'endomorphismes et de matrices}

\subsection{\(\K\)-algèbres}

\subsubsection{Définition}

\begin{defi}
Un ensemble \(A\) est appelé \(\K\)-algèbre quand \(A\) est à la fois un anneau et un \(\K\)-espace vectoriel, dont les multiplications sont compatibles.

Il y a donc trois lois dans une \(\K\)-algèbre :

\begin{itemize}
    \item une addition classique \(+\) ; \\
    \item une multiplication externe \(.\) ; \\
    \item une multiplication interne, compatible avec la précédente : \[\quantifs{\forall\paren{\lambda,a,b}\in\K\times A^2}\lambda.\paren{ab}=\paren{\lambda.a}b=a\paren{\lambda.b}.\]
\end{itemize}
\end{defi}

On qualifie les \(\K\)-algèbres par du vocabulaire des anneaux (algèbres intègres, algèbres principales, etc) ou des espaces vectoriels (algèbres de dimension finie, etc).

\begin{ex}
\begin{itemize}
    \item \(\K\) est lui-même une \(\K\)-algèbre, où les deux multiplications sont confondues ; \(\C\) est aussi une \(\R\)-algèbre de dimension \(2\). \\
    \item \(\poly\) est une \(\K\)-algèbre intègre, commutative et de dimension finie. \\
    \item Si \(I\) est un intervalle, \(\F{I}{\K}\) est une \(\K\)-algèbre commutative, non-intègre et de dimension finie. \\
    \item Si \(n\in\Ns\), \(\M{n}\) est une \(\K\)-algèbre de dimension \(n^2\) qui n'est ni intègre ni commutative. \\
    \item Si \(E\) est un \(\K\)-espace vectoriel, alors \(\Lendo{E}\) est une \(\K\)-algèbre de dimension finie ssi \(E\) l'est aussi qui n'est ni intègre ni commutative.
\end{itemize}
\end{ex}

\subsubsection{Polynômes d'éléments dans une \(\K\)-algèbre}

\begin{prop}\thlabel{prop:polynomeElementAlgebre}
Soient \(A\) une \(\K\)-algèbre et \(a\in A\).

Pour \(P=\sum_{i=0}^nc_iX^i\in\poly\), on pose \(P\paren{a}=\sum_{i=0}^nc_ia^i\).

L'application \(\fonctionlambda{\poly}{A}{P}{P\paren{a}}\) est alors un morphisme d'algèbres : \[\quantifs{\forall\paren{P,Q}\in\poly^2;\forall\lambda\in\K}\begin{dcases}
\paren{P+Q}\paren{a}=P\paren{a}+Q\paren{a} \\
\paren{PQ}\paren{a}=P\paren{a}Q\paren{a} \\
\paren{\lambda P}\paren{a}=\lambda P\paren{a}
\end{dcases}\]

De plus, on note \(\poly[\K][a]\) l'ensemble \(\accol{P\paren{a}\tq P\in\poly}\) : cet ensemble est stable par les lois de \(A\), on dit que c'est une sous-algèbre de \(A\).
\end{prop}

\subsection{Cas particulier des algèbres \(\Lendo{E}\) ou \(\M{n}\)}

\(E\) étant un \(\K\)-espace vectoriel, l'ensemble \(\Lendo{E}\) est une \(\K\)-algèbre. De même, \(n\) étant un entier non-nul, \(\M{n}\) est une \(\K\)-algèbre. Dans ces algèbres, on définit naturellement la notion de polynôme d'endomorphisme ou de matrice. Bien sûr, ces notions sont liées par choix d'une base de l'espace.

\begin{prop}
Soient \(E\) un \(\K\)-espace vectoriel de dimension \(n\), \(\fami{B}\) une base de \(E\) et \(f\in\Lendo{E}\) tel que \(\Mat{f}=A\).

Alors pour tout polynôme \(P\in\poly\), \(P\paren{f}\) a pour matrice \(P\paren{A}\) dans la base \(\fami{B}\).
\end{prop}

\begin{dem}
Comme \(A=\Mat{f}\), \(\quantifs{\tpt k\in\N}A^k=\Mat{f^k}\).

Donc \(\quantifs{\forall\paren{a_0,\dots,a_p}\in\K^p}\Mat{\sum_{i=0}^pa_if^i}=\sum_{i=0}^pa_iA^i\) \cad \[\quantifs{\forall P\in\poly}\Mat{P\paren{f}}=P\paren{A}.\]
\end{dem}

\begin{rem}
\begin{itemize}
    \item La \guillemets{multiplication} dans \(\Lendo{E}\) est la composition \(\rond\). Donc la deuxième propriété de la \thref{prop:polynomeElementAlgebre} doit être comprise comme suit : si \(f\in\Lendo{E}\), alors \(\paren{PQ}\paren{f}=P\paren{f}\rond Q\paren{f}\). \\
    \item Même si les multiplications dans \(\M{n}\) ou \(\Lendo{E}\) ne sont pas commutatives en général, on peut intervertir l'ordre des polynômes car la multiplication dans \(\poly\) est commutative : si \(A\in\M{n}\), \(\paren{PQ}\paren{A}=P\paren{A}Q\paren{A}=Q\paren{A}P\paren{A}\) ; si \(u\in\Lendo{E}\), \(\paren{PQ}\paren{u}=P\paren{u}\rond Q\paren{u}=Q\paren{u}\rond P\paren{u}\). \\
    \item Attention aux notations ! Si \(f\in\Lendo{E}\), \(x\in E\) et \(P\in\poly\), alors l'application de \(P\paren{f}\) au vecteur \(x\) se note \(P\paren{f}\paren{x}\) et pas \(P\paren{f\paren{x}}\), notation qui n'a aucun sens.
\end{itemize}
\end{rem}

\subsection{Polynôme annulateur d'une matrice ou d'un endomorphisme}

\begin{defi}
Soient \(n\in\Ns\) et \(A\in\M{n}\). On appelle polynôme annulateur de \(A\) tout polynôme non-nul \(P\in\poly\) tel que \(P\paren{A}=0\).

Soient \(E\) un \(\K\)-espace vectoriel et \(u\in\Lendo{E}\). On appelle polynôme annulateur de \(u\) tout polynôme non-nul \(P\in\poly\) tel que \(P\paren{u}=0\).
\end{defi}

\begin{rem}
Attention à ne pas confondre les notions : si \(P\) est un polynôme annulateur de la matrice \(A\) (on dit aussi que \(P\) annule \(A\) par abus de langage), on ne dit pas que \(A\) est une racine de \(P\) !

Une racine d'un polynôme est un nombre...

De même, si \(P\paren{u}=0\), on ne dit pas que \(u\) est une racine de \(P\), ça n'a aucun sens.
\end{rem}

\begin{defi}
Si \(A\in\M{n}\), alors l'ensemble \(\Ann{A}=\accol{P\in\poly\tq P\paren{A}=0}\) est appelé idéal annulateur de \(A\).

Si \(u\) est un endomorphisme d'un espace vectoriel \(E\), alors l'ensemble \(\Ann{u}=\accol{P\in\poly\tq P\paren{u}=0}\) est appelé idéal annulateur de \(u\).
\end{defi}

\begin{theo}
Soit \(A\in\M{n}\). Alors \(\Ann{A}\) est un sous-espace vectoriel de \(\poly\) stable par \(\times\). De plus, il existe un unique polynôme unitaire \(\mu_A\) tel que \(\Ann{A}=\mu_A\poly\).

Soient \(E\) un \(\K\)-espace vectoriel de dimension finie et \(u\in\Lendo{E}\). Alors \(\Ann{u}\) est un sous-espace vectoriel de \(\poly\) stable par \(\times\). De plus, il existe un unique polynôme unitaire \(\mu_u\) tel que \(\Ann{u}=\mu_u\poly\).
\end{theo}

\begin{dem}
\begin{itemize}
    \item Soient \(\paren{P,Q}\in\Ann{A}^2\) et \(\lambda\in\K\). \\\\ On a \[\paren{P+Q}\paren{A}=P\paren{A}+Q\paren{A}=0+0=0\] et \[\paren{PQ}\paren{A}=P\paren{A}Q\paren{A}=0\times0=0\] et \[\paren{\lambda P}\paren{A}=\lambda P\paren{A}=\lambda\times0=0.\] Donc \(P+Q\in\Ann{A}\), \(PQ\in\Ann{A}\) et \(\lambda P\in\Ann{A}\). \\\\ Remarque : on a même montré que \(\quantifs{\forall P\in\poly;\forall Q\in\Ann{A}}PQ\in\Ann{A}\). \\
    \item On montre d'abord l'existence d'un polynôme annulateur de \(A\) non-nul. \\\\ On considère la famille \(\paren{I_n,A,\dots,A^{n^2}}\) qui contient \(n^2+1\) éléments de \(\M{n}\), espace de dimension \(n^2\), et est donc liée : il existe \(\paren{a_0,\dots,a_{n^2}}\in\K^{n^2+1}\) tel que \[\paren{a_0,\dots,a_{n^2}}\not=\paren{0,\dots,0}\qquad\text{et}\qquad\sum_{i=0}^{n^2}a_iA^i=0.\] Alors le polynôme \(\sum_{i=0}^{n^2}a_iX^i\) est un polynôme annulateur de \(A\). \\
    \item On pose \(D=\accol{\deg P\tq P\in\Ann{A}\excluant\accol{0}}\). \\\\ D'après ce qui précède, \(D\) est une partie non-vide de \(\N\). \\\\ Donc \(D\) admet un minimum \(d\geq1\) d'après le principe fondamental de \(\N\), associé à un polynôme \(P\not=0\) tel que \(P\paren{A}=0\). \\\\ En notant \(\lambda\) le coefficient dominant de \(P\), le polynôme \(\mu=\dfrac{1}{\lambda}P\) est annulateur de \(A\) et unitaire. \\
    \item Montrons que \(\Ann{A}=\mu\poly\). \\\\ L'inclusion \(\mu\poly\subset\Ann{A}\) est triviale. \\\\ Soit \(P\in\Ann{A}\). On effectue la division euclidienne de \(P\) par \(\mu\) : il existe \(\paren{Q,R}\in\poly^2\) tel que \[P=Q\mu+R\qquad\text{et}\qquad\deg R<\deg\mu=d.\] Donc \(P\paren{A}=Q\paren{A}\mu\paren{A}+R\paren{A}\). \\\\ Or \(P\paren{A}=\mu\paren{A}=0\) donc \(R\paren{A}=0\) \ie \(R\in\Ann{A}\). \\\\ Par définition de \(d\), on a \(R=0\). \\\\ Donc \(P\) est un multiple de \(\mu\). \\\\ D'où \(\Ann{A}\subset\mu\poly\). \\\\ Donc \(\Ann{A}=\mu\poly\). \\
    \item Montrons maintenant l'unicité de \(\mu\). \\\\ Si \(\nu\) est un polynôme annulateur de \(A\) de degré \(d\) et unitaire, alors \(\Ann{A}=\nu\poly=\mu\poly\). \\\\ Donc \(\nu\divise\mu\) et \(\mu\divise\nu\). \\\\ Or \(\mu\) et \(\nu\) sont unitaires donc \(\mu=\nu\).
\end{itemize}
\end{dem}

\begin{rem}
Ce dernier résultat est faux en dimension infinie. Contre-exemple : l'endomorphisme \(u:\poly\to\poly\) défini par \(u\paren{P}=XP\).
\end{rem}

\begin{defi}
On appelle polynôme minimal d'une matrice carrée \(A\) le polynôme \(\mu_A\) précédent (noté aussi parfois \(\pi_A\)). C'est le polynôme unitaire de degré minimal qui annule \(A\).

On appelle polynôme minimal d'un endomorphisme \(u\) en dimension finie le polynôme \(\mu_u\) précédent (noté aussi parfois \(\pi_u\)). C'est le polynôme unitaire de degré minimal qui annule \(u\).
\end{defi}

Autrement dit, on a l'équivalence : pour tout \(P\in\poly\), \[P\paren{u}=0\ssi\mu_u\divise P.\]

De même, on a l'équivalence : pour tout \(P\in\poly\), \[P\paren{A}=0\ssi\mu_A\divise P.\]

Les polynômes annulateurs sont donc les multiples des polynômes minimaux.

On verra plus tard qu'on peut trouver des polynômes annulateurs de plus petits degrés que ceux donnés par le théorème précédent.

En général, il est souvent pénible de calculer le polynôme minimal. En pratique, on se contente de trouver des polynômes annulateurs de degrés pas trop grands (et souvent, il s'agit du polynôme minimal).

\begin{rem}
Si \(A\in\M{n}\) et \(\K\prim\) est un sous-corps de \(\C\) qui contient \(\K\) (on dit que \(\K\prim\) est une extension de \(\K\)), alors le polynôme minimal de \(A\), vue comme matrice de \(\M{n}[\K\prim]\), est a priori différent de celui de \(A\) vue comme matrice de \(\M{n}\). On peut seulement affirmer pour l'instant que \(\mu_{A\,\K\prim}\) divise \(\mu_{A\,\K}\).

En fait, on montre plus loin que le polynôme minimal ne dépend pas du corps \(\K\).
\end{rem}

\subsection{Utilisation pratique d'un polynôme annulateur}

\subsubsection{Calcul de l'inverse}

\begin{prop}\thlabel{prop4.15}
Soient \(n\in\Ns\) et \(A\in\M{n}\).

Alors \(A\) est inversible ssi \(A\) possède un polynôme annulateur \(P\) tel que \(0\) ne soit pas racine de \(P\). Dans ce cas, \(A\inv\) est un polynôme en \(A\).

De même, soient \(E\) un \(\K\)-espace vectoriel et \(f\in\Lendo{E}\).

Alors si \(f\) possède un polynôme annulateur \(P\) tel que \(0\) ne soit pas racine de \(P\), \(f\) est un automorphisme de \(E\). Dans ce cas, \(f\inv\) est un polynôme en \(f\). La réciproque est vraie si \(E\) est de dimension finie.
\end{prop}

\begin{dem}
\imprec

On suppose que \(A\) possède un polynôme annulateur \(P\) tel que \(P\paren{0}\not=0\).

En notant \(P=\sum_{i=0}^pa_iX^i\), on a \(\sum_{i=0}^pa_iA^i=0\) et \(a_0\not=0\).

Donc \(a_0I_n+a_1A+\dots+a_pA^p=0\).

Donc \(a_0I_n=-a_1A-\dots-a_pA^p\).

Donc \(I_n=A\underbrace{\paren{\dfrac{-a_1}{a_0}I_n-\dots-\dfrac{a_p}{a_0}A^{p-1}}}_{B}\).

On a trouvé une matrice \(B\in\M{n}\) telle que \(AB=I_n=BA\).

Donc \(A\) est inversible et \(A\inv=B=Q\paren{A}\) où \(Q=\sum_{i=0}^{p-1}\dfrac{-a_{i+1}}{a_0}X^i\).

\impdir

On suppose \(A\) inversible. On veut montrer que \(\mu_A\paren{0}\not=0\).

Par l'absurde, on suppose \(\mu_A\paren{0}=0\).

On a \(\mu_A=a_1X+\dots+X^p\) et \(\mu_A\paren{A}=0\).

Donc \(a_1A+\dots+A^p=0\).

\(A\) étant inversible, on multiplie par \(A\inv\) et on obtient \(a_1I_n+a_2A+\dots+A^{p-1}=0\).

Donc \(a_1+a_2X+\dots+X^{p-1}\) est annulateur de \(A\) et de degré strictement inférieur à celui de \(\mu_A\) : contradiction.

Donc \(\mu_A\paren{0}\not=0\).
\end{dem}

\begin{exo}~\\
On pose \(A=\begin{pmatrix}
1 & 1 & 1 \\
0 & 1 & 1 \\
0 & 0 & 2
\end{pmatrix}\). Déterminez un polynôme annulateur de \(A\), montrez que \(A\) est inversible et calculez \(A\inv\).
\end{exo}

\begin{corr}~\\
On a \(A^2=\begin{pmatrix}
1 & 2 & 4 \\
0 & 1 & 3 \\
0 & 0 & 4
\end{pmatrix}\) et \(A^3=\begin{pmatrix}
1 & 3 & 11 \\
0 & 1 & 7 \\
0 & 0 & 8
\end{pmatrix}\).

Donc on a \[\begin{aligned}
A^3=xA^2+yA+zI_3&\ssi\begin{dcases}
1=x+y+z \\
8=4x+2y+z \\
7=3x+y \\
11=4x+y \\
3=2x+y
\end{dcases} \\
&\ssi\begin{dcases}
x=4 \\
y=-5 \\
z=2
\end{dcases}
\end{aligned}\]

Donc \(A^3=4A^2-5A+2I_3\).

Donc \(P=X^3-4X^2+5X-2\) est un polynôme annulateur de \(A\).

On a \(P\paren{0}\not=0\) donc \(A\) est inversible.

On obtient \[A\inv=\dfrac{1}{2}\paren{A^2-4A+5I_3}=\begin{pmatrix}
1 & -1 & 0 \\
0 & 1 & -\nicefrac{1}{2} \\
0 & 0 & \nicefrac{1}{2}
\end{pmatrix}.\]
\end{corr}

\begin{exo}
Soient \(E\) un \(\K\)-espace vectoriel et \(p\in\Lendo{E}\) un projecteur. Déterminez un polynôme annulateur de \(p\).

Soit \(\lambda\in\K\). On pose \(f=p-\lambda\id{E}\). Déterminez un polynôme annulateur de \(f\) et vérifiez que \(f\) est un automorphisme pour presque toutes les valeurs de \(\lambda\) ; dans ce cas, calculez son inverse.
\end{exo}

\begin{corr}
\(p\) est un projecteur donc \(p^2=p\) donc \(X^2-X\) est un polynôme annulateur de \(p\).

Donc le polynôme annulateur minimal de \(p\) est un diviseur unitaire de \(X^2-X\) : \(X^2-X\), \(X\) (auquel cas \(p=0\)) ou \(X-1\) (auquel cas \(p=\id{E}\)).

On a \(f=p-\lambda\id{E}\) donc \[\begin{aligned}
f^2&=p^2-2\lambda p+\lambda^2\id{E} \\
&=\paren{1-2\lambda}p+\lambda^2\id{E} \\
&=\paren{1-2\lambda}f+\lambda\paren{1-2\lambda}\id{E}+\lambda^2\id{E} \\
&=\paren{1-2\lambda}f+\paren{-\lambda^2+\lambda}\id{E}.
\end{aligned}\]

Donc \(P=X^2+\paren{2\lambda-1}X+\paren{\lambda^2-\lambda}\) est annulateur de \(f\).

Si \(\lambda\not=0\) et \(\lambda\not=1\) alors \(\lambda^2-\lambda=P\paren{0}=0\).

Donc d'après la \thref{prop4.15}, \(f\in\GL{}[E]\).

De plus, \(f\inv=\dfrac{1}{\lambda-\lambda^2}\paren{f+\paren{2\lambda-1}\id{E}}\).

Si \(\lambda=0\) alors \(f=p\) est un automorphisme ssi \(p=\id{E}\).

Si \(\lambda=1\) alors \(f=p-\id{E}\) est un automorphisme ssi \(p=0\).
\end{corr}

\subsubsection{Calcul de puissances}

\begin{prop}
Soient \(n\in\Ns\) et \(A\in\M{n}\). On choisit un polynôme annulateur \(P\) de la matrice \(A\).

Alors \(\quantifs{\tpt p\in\N}A^p=R_p\paren{A}\) où \(R_p\) est le reste de la division euclidienne de \(X^p\) par \(P\).

De même, soient \(E\) un \(\K\)-espace vectoriel et \(f\in\Lendo{E}\) qui possède un polynôme annulateur \(P\).

Alors \(\quantifs{\tpt p\in\N}f^p=R_p\paren{f}\) où \(R_p\) est le reste de la division euclidienne de \(X^p\) par \(P\).
\end{prop}

\begin{dem}
On effectue la division euclidienne de \(X^p\) par \(P\) : \(X^p=PQ+R_p\) et \(\deg R_p<\deg P\).

Alors \(A^p=P\paren{A}Q\paren{A}+R_p\paren{A}=R_p\paren{A}\) car \(P\paren{A}=0\).
\end{dem}

Conséquence :

\begin{itemize}
    \item si \(A\) possède un polynôme annulateur de degré \(a\), alors \(\poly[\K][A]=\accol{P\paren{A}\tq P\in\polydeg{a-1}}\) ; \\
    \item si \(f\) possède un polynôme annulateur de degré \(a\), alors \(\poly[\K][f]=\accol{P\paren{f}\tq P\in\polydeg{a-1}}\).
\end{itemize}

\begin{prop}
Si \(p\) est le degré du polynôme minimal d'une matrice \(A\), alors \(\dim\poly[\K][A]=p\) et \(\paren{I_n,A,\dots,A^{p-1}}\) est une base de \(\poly[\K][A]\).

Si \(p\) est le degré du polynôme minimal d'un endomorphisme \(f\) d'un espace vectoriel \(E\), alors \(\dim\poly[\K][f]=p\) et \(\paren{\id{E},f,\dots,f^{p-1}}\) est une base de \(\poly[\K][f]\).
\end{prop}

\begin{dem}
On note \(p=\deg\mu_A\).

Soit \(B\in\poly[\K][A]\).

Il existe \(P\in\poly\) tel que \(B=P\paren{A}\).

On effectue la division euclidienne de \(P\) par \(\mu_A\) : \(P=Q\mu_A+R\) avec \(\deg R<p\).

Donc \(P\paren{A}=Q\paren{A}\mu_A\paren{A}+R\paren{A}=R\paren{A}\in\Vect{I_n,A,\dots,A^{p-1}}\).

Donc \(\poly[\K][A]\subset\Vect{I_n,A,\dots,A^{p-1}}\).

L'inclusion réciproque est immédiate.

Donc \(\poly[\K][A]=\Vect{I_n,A,\dots,A^{p-1}}\).

Si \(\paren{I_n,A,\dots,A^{p-1}}\) est liée, alors il existe \(\paren{a_0,\dots,a_{p-1}}\not=\paren{0,\dots,0}\) tel que \(\sum_{i=0}^{p-1}a_iA^i=0\).

En posant \(P=\sum_{i=0}^{p-1}a_iX^i\), on a un polynôme annulateur de \(A\) de degré strictement inférieur à \(p=\deg\mu_A\) : contradiction.

Donc \(\paren{I_n,A,\dots,A^{p-1}}\) est libre et est donc une base de \(\poly[\K][A]\) et \(\dim\poly[\K][A]=p\).
\end{dem}

\begin{exo}
Soient \(E\) un espace vectoriel et \(\paren{p,q}\in\Lendo{E}^2\) deux projecteurs tels que \(p+q=\id{E}\). Vérifiez que \(p\rond q=q\rond p=0\). Déterminez un polynôme annulateur de \(f=2p+3q\). Donnez une expression générale de \(f^k\) en fonction de \(f\) et \(k\).
\end{exo}

\begin{corr}
On a \(p+q=\id{E}\) donc \(p\rond p+p\rond q=p\) donc \(p\rond q=0\) car \(p\rond p=p\).

De même, \(q\rond p=0\).

On a donc \(f^2=4p^2+6pq+6qp+9q^2=4p+9q\).

Donc \[\begin{aligned}
f^2=xf+y\id{E}&\ssi\begin{dcases}
4=2x+y \\
9=3x+y
\end{dcases} \\
&\ssi\begin{dcases}
5=x \\
-6=y
\end{dcases}
\end{aligned}\]

Donc \(P=X^2-5X+6\) est un polynôme annulateur de \(f\) dont les racines sont \(2\) et \(3\).

Soit \(k\in\N\).

On effectue la division euclidienne de \(X^k\) par \(P\) : \[X^k=\paren{X^2-5X+6}Q+a_kX+b_k.\]

En appliquant en \(2\) et en \(3\), on obtient \[\begin{dcases}
2a_k+b_k=2^k \\
3a_k+b_k=3^k
\end{dcases}\qquad\text{donc}\qquad\begin{dcases}
a_k=3^k-2^k \\
b_k=3\times2^k-2\times3^k
\end{dcases}\]

Donc \(f^k=a_kf+b_k\id{E}=\paren{3^k-2^k}f+\paren{3\times2^k-2\times3^k}\id{E}\).
\end{corr}

\begin{rem}
Plus généralement, si \(P\) est annulateur de \(f\) et de degré \(N\) à \(N\) racines simples \(\omega_1,\dots,\omega_N\).

Alors \(X^k=PQ+c_{N-1}X^{N-1}+\dots+c_1X+c_0\).

Alors on a un système linéaire à \(N\) équations et \(N\) inconnues \[\begin{dcases}
c_0+c_1\omega_1+c_2\omega_1^2+\dots+c_{N-1}\omega_1^{N-1}=\omega_1^k \\
\vdots \\
c_0+c_1\omega_N+c_2\omega_N^2+\dots+c_{N-1}\omega_N^{N-1}=\omega_N^k
\end{dcases}\]

Ce système a pour déterminant \[\begin{vmatrix}
1 & \omega_1 & \omega_1^2 & \dots & \omega_1^{N-1} \\
 & & \vdots & & \vdots \\
1 & \omega_N & \omega_N^2 & \dots & \omega_N^{N-1}
\end{vmatrix}=\prod_{1\leq i<j\leq n}\paren{\omega_j-\omega_i}\not=0\] et admet donc une unique solution.
\end{rem}

\begin{exo}~\\
On pose \(A=\begin{pmatrix}
2 & -4 & -5 \\
-1 & 2 & 2 \\
1 & -2 & -2
\end{pmatrix}\). Vérifiez que \(P=X^3-2X^2+X\) est un polynôme annulateur de \(A\). Donnez une expression générale de \(A^p\) en fonction de \(A\) et \(p\).
\end{exo}

\begin{corr}~\\
On a \(A^2=\begin{pmatrix}
3 & -6 & -8 \\
-2 & 4 & 5 \\
2 & -4 & -5
\end{pmatrix}\) et \(A^3=\begin{pmatrix}
4 & -8 & -11 \\
-3 & 6 & 8 \\
3 & -6 & -8
\end{pmatrix}\) donc \(P=X^3-2X^2+X=X\paren{X-1}^2\) est annulateur de \(A\).

Soit \(k\in\N\).

On effectue la division euclidienne de \(X^k\) par \(P\) : \[X^k=X\paren{X-1}^2Q+a_kX^2+b_kX+c_k.\]

En évaluant en \(0\) et \(1\), on obtient \[\begin{dcases}
c_k=0^k \\
a_k+b_k+c_k=1^k \\
2a_k+b_k=k1^k
\end{dcases}\]

Or \(1\) est racine double donc \(P\prim\paren{1}=0\) donc \(2a_k+b_k=k\).

Donc, si \(k\not=0\), on a \[\begin{dcases}
c_k=0 \\
a_k+b_k=1 \\
2a_k+b_k=k
\end{dcases}\qquad\text{donc}\qquad\begin{dcases}
a_k=k-1 \\
b_k=2-k \\
c_k=0
\end{dcases}\]

Donc \(\quantifs{\tpt k\in\Ns}A^k=\paren{k-1}A^2+\paren{2-k}A\).
\end{corr}

\begin{cor}
Si \(\K\prim\) est une extension de \(\K\), alors pour toute matrice \(A\in\M{n}\), ses polynômes minimaux relativement à \(\K\) et \(\K\prim\) sont égaux : \(\mu_{A\,\K}=\mu_{A\,\K\prim}\).

Autrement dit, le polynôme minimal ne dépend pas du corps \(\K\).
\end{cor}

\begin{dem}
Soit \(A\in\M{n}\).

On note \(\Ann[\K]{A}=\accol{P\in\poly\tq P\paren{A}=0}\) et \(\Ann[\K\prim]{A}=\accol{P\in\poly[\K\prim]\tq P\paren{A}=0}\).

Soit \(P\in\Ann[\K]{A}\).

Comme \(\K\subset\K\prim\), on a \(P\in\Ann[\K\prim]{A}\).

Donc \(\mu_{A\,\K}\poly=\Ann[\K]{A}\subset\Ann[\K\prim]{A}=\mu_{A\,\K\prim}\poly[\K\prim]\).

Donc \(\mu_{A\,\K}\in\Ann[\K\prim]{A}\) donc \(\mu_{A\,\K\prim}\divise\mu_{A\,\K}\).

Donc \(p\prim=\deg\mu_{A\,\K\prim}\leq\deg\mu_{A\,\K}=p\).

Or \(\paren{I_n,A,\dots,A^{p-1}}\) est libre dans le \(\K\)-espace vectoriel \(\poly[\K][A]\) et \(\paren{I_n,A,\dots,A^{p\prim-1}}\) est libre dans le \(\K\prim\)-espace vectoriel \(\poly[\K\prim][A]\).

On veut montrer que \(\paren{I_n,A,\dots,A^{p-1}}\) est libre dans le \(\K\prim\)-espace vectoriel \(\poly[\K\prim][A]\).

Par l'absurde, si cette famille est liée, alors il existe \(\paren{\lambda_0,\dots,\lambda_{p-1}}\in\paren{\K\prim}^p\) tel que \[\begin{dcases}
\paren{\lambda_0,\dots,\lambda_{p-1}}\not=\paren{0,\dots,0} \\
\sum_{i=0}^{p-1}\lambda_iA^i=0
\end{dcases}\]

On pose \(q=\max\accol{i\in\interventierii{0}{p-1}\tq\lambda_i\not=0}\).

On a alors \(A^q=-\sum_{i=0}^{q-1}\dfrac{\lambda_i}{\lambda_q}A^i\).

Cette égalité est interprétable en un système linéaire à \(n^2\) équations dont les \guillemets{inconnues} sont \(\dfrac{-\lambda_i}{\lambda_q}\) et les coefficients sont les coefficients de \(A^q,\dots,I_n\) qui sont tous dans \(\K\).

En résolvant ce système linéaire, on obtient des solutions qui sont dans \(\K\).

Donc on obtient une relation de dépendance linéaire entre les matrices \(\paren{I_n,\dots,A^q,\dots,A^{p-1}}\) à coefficients dans \(\K\) : contradiction car \(\paren{I_n,\dots,A^{p-1}}\) est libre.

Donc \(p\leq\dim_{\K\prim}\poly[\K\prim][A]=p\prim\).

D'où \(p=p\prim\).

Donc, comme \(\mu_{A\,\K\prim}\divise\mu_{A\,\K}\) et que ces polynômes sont unitaires, on a \[\mu_{A\,\K\prim}=\mu_{A\,\K}.\]
\end{dem}

\section{Matrices semblables, trace}

\subsection{Trace d'une matrice}

\begin{defi}
Soit \(A\in\M{n}\). On appelle trace de \(A\) la somme de ses coefficients diagonaux : \[\tr A=\sum_{i=1}^na_{i\,i}.\]
\end{defi}

L'application trace vérifie de remarquables propriétés.

\begin{prop}
\begin{itemize}
    \item La trace est une forme linéaire sur \(\M{n}\). \\
    \item \(\quantifs{\Tpt A\in\M{n}}\tr\paren{\trans{A}}=\tr A\). \\
    \item \(\quantifs{\Tpt\paren{A,B}\in\M{n}^2}\tr\paren{AB}=\tr\paren{BA}\).
\end{itemize}
\end{prop}

\subsection{Matrices semblables}

\begin{defi}
Soient \(A,B\in\M{n}\).

On dit que \(A\) et \(B\) sont semblables quand il existe \(P\in\GL{n}\) telle que \(B=P\inv AP\).
\end{defi}

\begin{prop}
La relation de similitude entre matrices de \(\M{n}\) est une relation d'équivalence.
\end{prop}

La relation de similitude est une relation très contraignante. Il n'existe pas de caractérisation simple de la similitude entre deux matrices carrées : savoir si deux matrices sont semblables est un problème difficile.

D'après la formule de changement de base, on a immédiatement le résultat suivant.

\begin{prop}
Deux matrices de \(\M{n}\) sont semblables ssi elle représentent un même endomorphisme dans des bases différentes. La matrice \(P\) est la matrice de passage d'une base à l'autre.
\end{prop}

\begin{cor}
Deux matrices semblables ont même rang, même trace et même déterminant.
\end{cor}

Mais c'est loin d'être suffisant pour être semblables.

\begin{prop}
Si \(A\) et \(B\) sont deux matrices semblables, alors pour tout polynôme \(P\in\poly\), \(P\paren{A}\) et \(P\paren{B}\) sont semblables avec la même matrice de passage.
\end{prop}

\begin{dem}
Si \(A\) et \(B\) sont semblables, il existe \(Q\in\GL{n}\) telle que \(B=Q\inv AQ\).

Alors, par récurrence sur \(k\in\N\), on a \(B^k=Q\inv A^kQ\).

Puis, par combinaison linéaire, \(\quantifs{\tpt P\in\poly}P\paren{B}=Q\inv P\paren{A}Q\).
\end{dem}

\subsection{Trace d'un endomorphisme}

\begin{prop}
Soient \(E\) un \(\K\)-espace vectoriel de dimension finie et \(f\in\Lendo{E}\).

Toutes les matrices carrées représentant \(f\) ont la même trace. Cette trace ne dépend donc pas du choix de la base dans laquelle on écrit la matrice de \(f\), elle ne dépend que de \(f\) : on l'appelle trace de \(f\) et on la note \(\tr f\).
\end{prop}

On peut alors reformuler les résultats sur la trace d'une matrice.

\begin{prop}
\begin{itemize}
    \item La trace est une forme linéaire sur \(\Lendo{E}\). \\
    \item \(\quantifs{\Tpt\paren{u,v}\in\Lendo{E}^2}\tr\paren{u\rond v}=\tr\paren{v\rond u}\).
\end{itemize}
\end{prop}

\section{Opérations par blocs}

\subsection{Cas général}

Soit \(\paren{n,p}\in\paren{\Ns}^2\). On fixe deux entiers \(k,l\) tels que \(1\leq k\leq n-1\) et \(1\leq l\leq p-1\).

À toute matrice \(M\in\M{n\,p}\), on associe quatre matrices obtenues en découpant la matrice en blocs : \[M=\begin{pmatrix}
A & B \\
C & D
\end{pmatrix}\] où \(A=\paren{m_{i\,j}}_{\substack{1\leq i\leq k \\ 1\leq j\leq l}}\), \(B=\paren{m_{i\,j}}_{\substack{1\leq i\leq k \\ l+1\leq j\leq p}}\), \(C=\paren{m_{i\,j}}_{\substack{k+1\leq i\leq n \\ 1\leq j\leq l}}\) et \(D=\paren{m_{i\,j}}_{\substack{k+1\leq i\leq n \\ l+1\leq j\leq p}}\).

Cette décomposition par blocs permet de faire des calculs formellement comme s'il s'agissait de nombres.

\begin{prop}~\\
Soient \(M=\begin{pmatrix}
A & B \\
C & D
\end{pmatrix}\) et \(M\prim=\begin{pmatrix}
A\prim & B\prim \\
C\prim & D\prim
\end{pmatrix}\) deux matrices de même taille décomposées de la même façon en blocs et \(\lambda\in\K\).

Alors \(M+M\prim=\begin{pmatrix}
A+A\prim & B+B\prim \\
C+C\prim & D+D\prim
\end{pmatrix}\) et \(\lambda M=\begin{pmatrix}
\lambda A & \lambda B \\
\lambda C & \lambda D
\end{pmatrix}\).
\end{prop}

\begin{prop}~\\
Soient \(M=\begin{pmatrix}
A & B \\
C & D
\end{pmatrix}\) et \(M\prim=\begin{pmatrix}
A\prim & B\prim \\
C\prim & D\prim
\end{pmatrix}\) deux matrices telles que le produit \(MM\prim\) existe et décomposées en blocs.

Alors, sous réserve que les blocs soient de tailles compatibles pour la multiplication, on a \[MM\prim=\begin{pmatrix}
AA\prim+BC\prim & AB\prim+BD\prim \\
CA\prim+DC\prim & CB\prim+DD\prim
\end{pmatrix}.\]
\end{prop}

\begin{rem}
\begin{itemize}
    \item Comme les symboles mis en jeu ne sont pas des nombres mais des matrices, il est indispensable de respecter l'ordre dans les produits. \\
    \item On peut généraliser à un nombre quelconque de blocs, pas forcément deux en ligne ou en colonne.
\end{itemize}
\end{rem}

\subsection{Cas particuliers des matrices carrées}

Si \(M\) est une matrice de \(\M{n}\), alors avec les mêmes notations, on choisit toujours \(A\) et \(D\) carrées elles aussi. Dans ce paragraphe, on suppose que c'est le cas.

\begin{defi}
On dit que \(M\) est triangulaire supérieure par blocs quand il existe des matrices carrées \(A_1,\dots,A_k\) telles que \(M\) soit de la forme \[M=\begin{pmatrix}
A_1 & ? & ? & \dots & ? \\
0 & A_2 & ? & \dots & ? \\
\vdots &  & \ddots &  & \vdots \\
0 & \dots & 0 & A_{k-1} & ? \\
0 & \dots & 0 & 0 & A_k
\end{pmatrix}.\]
\end{defi}

On définit de même la notion de matrice triangulaire inférieure par blocs.

\begin{defi}
On dit que \(M\) est diagonale par blocs quand il existe des matrices carrées \(A_1,\dots,A_k\) telles que \(M\) soit de la forme \[M=\begin{pmatrix}
A_1 & 0 & 0 & \dots & 0 \\
0 & A_2 & 0 & \dots & 0 \\
\vdots &  & \ddots &  & \vdots \\
0 & \dots & 0 & A_{k-1} & 0 \\
0 & \dots & 0 & 0 & A_k
\end{pmatrix}.\]
\end{defi}

Les résultats sur les matrices triangulaires ou diagonales restent valables par blocs : la somme et le produit de deux matrices triangulaires supérieures par blocs de mêmes tailles l'est encore, et de même pour les matrices triangulaires inférieures par blocs et les matrices diagonales par blocs.

Une conséquence est qu'une matrice \(M\) triangulaire par blocs est inversible ssi tous les blocs diagonaux sont inversibles.

Dans ce cas, l'inverse de \(M\) est triangulaire par blocs et ses blocs diagonaux sont les inverses des blocs diagonaux de \(M\).

En particulier, l'inverse d'une matrice \(M\) diagonale par blocs est la matrice diagonale par blocs dont les blocs diagonaux sont les inverses de ceux de \(M\).

De plus, le déterminant d'une matrice triangulaire par blocs est le produit des déterminants des blocs diagonaux.

\subsection{Interprétation des blocs}

\begin{defi}
Soient \(E\) un \(\K\)-espace vectoriel, \(f\in\Lendo{E}\) et \(F\) un sous-espace vectoriel de \(E\).

On dit que \(F\) est stable par \(f\) quand \(f\paren{F}\subset F\), \ie \(\quantifs{\tpt x\in F}f\paren{x}\in F\).
\end{defi}

Dans ce cas, on peut définir une application \(\phi\) de \(F\) dans \(F\) en posant \(\quantifs{\tpt x\in F}\phi\paren{x}=f\paren{x}\).

Il est facile de vérifier que \(\phi\) est un endomorphisme de \(F\), appelé endomorphisme induit par \(f\) dans \(F\).

\begin{ex}
Si \(g\) est un endomorphisme de \(E\) qui commute avec \(f\) (\ie \(fg=gf\)), alors \(\ker g\) et \(\Im g\) sont stables par \(f\).
\end{ex}

\begin{prop}
Soient \(E\) un \(\K\)-espace vectoriel de dimension \(n\), \(f\in\Lendo{E}\) et \(F\) un sous-espace vectoriel de \(E\) de dimension \(p\).

Si \(F\) est stable par \(f\), alors il existe une base de \(E\) dans laquelle la matrice de \(f\) est triangulaire supérieure par bloc, le premier bloc étant de taille \(\paren{p,p}\) : \[\Mat{f}=\begin{pmatrix}
A & B \\
0 & D
\end{pmatrix}\text{ et }A\in\M{p}.\]

Réciproquement, si \(f\) possède une matrice de cette forme, alors le sous-espace vectoriel engendré par les \(p\) premiers vecteurs est stable par \(f\).
\end{prop}

\begin{dem}
On choisit une base de \(E\) adaptée à \(F\) : \(\fami{B}=\paren{\underbrace{e_1,\dots,e_p}_{\text{base de }F},e_{p+1},\dots,e_n}\).

Pour tout \(j\in\interventierii{1}{p}\), en notant \(f\paren{e_j}=\paren{a_{1\,j},\dots,a_{n\,j}}_{\fami{B}}\), on a \[\begin{aligned}
f\paren{e_j}\in F&\ssi f\paren{e_j}\in\Vect{e_1,\dots,e_p} \\
&\ssi\quantifs{\forall i\geq p+1}a_{i\,j}=0 \\
&\ssi f\paren{e_j}=\paren{a_{1\,j},\dots,a_{p\,j},0,\dots,0}_{\fami{B}}.
\end{aligned}\]

Alors, si \(F\) est stable par \(f\), on a \[\Mat{f}=\begin{pmatrix}
a_{1\,1} & \dots & a_{1\,p} & ? & \dots & ? \\
\vdots &  & \vdots & \vdots &  & \vdots \\
a_{p\,1} & \dots & a_{p\,p} & \vdots &  & \vdots \\
0 & \dots & 0 & \vdots &  & \vdots \\
\vdots &  & \vdots & \vdots &  & \vdots \\
0 & \dots & 0 & ? & \dots & ?
\end{pmatrix}.\]

Et réciproquement.
\end{dem}

\begin{prop}
Soient \(E\) un \(\K\)-espace vectoriel de dimension \(n\) et \(f\in\Lendo{E}\).

Si \(F_1,\dots,F_k\) sont des sous-espaces vectoriels supplémentaires stables par \(f\) de dimensions respectives \(p_1,\dots,p_k\), alors il existe une base de \(E\) dans laquelle la matrice de \(f\) est diagonale par blocs, la taille du \(i\)-ème bloc étant \(\paren{p_i,p_i}\) : \[\Mat{f}=\begin{pmatrix}
A_1 & 0 & 0 & \dots & 0 \\
0 & A_2 & 0 & \dots & 0 \\
\vdots &  & \ddots &  & \vdots \\
0 & \dots & 0 & A_{k-1} & 0 \\
0 & \dots & 0 & 0 & A_k
\end{pmatrix}.\]

Réciproquement, si \(f\) possède une matrice dans une certaine base qui est diagonale par blocs et contenant \(k\) blocs carrés, alors il existe \(k\) sous-espaces vectoriels \(F_1,\dots,F_k\) stables par \(f\) et supplémentaires dans \(E\).
\end{prop}
