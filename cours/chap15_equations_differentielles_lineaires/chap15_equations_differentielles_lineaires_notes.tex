\chapter{Équations différentielles linéaires}

\minitoc

Dans tout le cours, \(I\) désigne un intervalle de \(\R\) contenant au moins deux points, \(n\) désigne un entier naturel non-nul, \(\K=\R\) ou \(\C\), et \(E\) est un espace vectoriel normé de dimension finie \(n\).

Si \(f\) est un endomorphisme de \(E\) et \(v\) un vecteur de \(E\), on note \(f.v\) l'image de \(v\) par \(f\), au lieu de la traditionnelle notation \(f\paren{v}\). Sans cette notation, on rencontrerait des notations lourdes comme \(f\paren{t}\paren{v}\) ou \(f\paren{t}\paren{v\paren{t}}\), qui seront donc avantageusement remplacées par \(f\paren{t}.v\) ou \(f\paren{t}.v\paren{t}\).

\section{Équations et systèmes différentiels linéaires du premier ordre}

\subsection{Généralités}

\begin{defi}
On appelle équation différentielle linéaire (du premier ordre) toute équation différentielle de la forme \[x\prim=a\paren{t}.x+b\paren{t}\] où

\begin{itemize}
    \item \(a\) est une fonction continue de \(I\) dans \(\Lendo{E}\) \\
    \item \(b\) est une fonction continue de \(I\) dans \(E\) (appelée second membre) \\
    \item \(x\) est la fonction inconnue dérivable de \(I\) dans \(E\) et \(x\prim\) sa dérivée.
\end{itemize}
\end{defi}

\begin{defi}
Avec les notations précédentes, on appelle solution (sur \(I\)) de l'équation différentielle toute application \(x\) dérivable sur \(I\) qui vérifie \(\quantifs{\forall t\in I}x\prim\paren{t}=a\paren{t}.x\paren{t}+b\paren{t}\).
\end{defi}

\begin{rem}
Si le second membre \(b\) peut s'écrire sous la forme \(b=b_1+b_2\), alors en additionnant les solutions de l'équation \(x\prim=a\paren{t}.x+b_1\paren{t}\) avec celles de l'équation \(x\prim=a\paren{t}.x+b_2\paren{t}\), on obtient des solutions de \(x\prim=a\paren{t}.x+b\paren{t}\).

On appelle cette idée le principe de superposition.
\end{rem}

Comme \(E\) est de dimension finie \(n\), si on choisit une base \(\fami{B}\), alors en exprimant les coordonnées des vecteurs et les matrices des endomorphismes dans la base \(\fami{B}\), on obtient une traduction matricielle de l'équation différentielle précédente.

\begin{defi}
On appelle système différentiel linéaire (du premier ordre) toute équation différentielle de la forme \[X\prim=A\paren{t}X+B\paren{t}\] où

\begin{itemize}
    \item \(A\) est une fonction continue de \(I\) dans \(\M{n}\) \\
    \item \(B\) est une fonction continue de \(I\) dans \(\M{n\,1}=\K^n\) \\
    \item \(X\) est la fonction inconnue dérivable de \(I\) dans \(\M{n\,1}\) et \(X\prim\) sa dérivée.
\end{itemize}
\end{defi}

\begin{ex}
\begin{itemize}
    \item Le système \(\begin{dcases}
        x\prim=2x+3y \\
        y\prim=-x+4y
    \end{dcases}\) est un système différentiel linéaire où \(A\) est la fonction constante égale à \(\begin{pmatrix}
        2 & 3 \\
        -1 & 4
    \end{pmatrix}\) et \(B=0\). \\
    \item Le système \(\begin{dcases}
        x\prim=t^2x+\e{t}y+\sin t \\
        y\prim=-tx+\cos\paren{t}y-1
    \end{dcases}\) est un système différentiel linéaire où \(A\) est la fonction \(t\mapsto\begin{pmatrix}
        t^2 & \e{t} \\
        -t & \cos t
    \end{pmatrix}\) et \(B\) est la fonction \(t\mapsto\dcoords{\sin t}{-1}\).
\end{itemize}
\end{ex}

\begin{defi}
Avec les notations précédentes, on appelle solution (sur \(I\)) du système différentiel toute application \(X\) dérivable sur \(I\) qui vérifie \(\quantifs{\forall t\in I}X\prim\paren{t}=A\paren{t}X\paren{t}+B\paren{t}\).
\end{defi}

\begin{prop}
Les solutions d'une telle équation différentielle ou d'un tel système différentiel linéaire sont au moins de classe \(\classe{1}\) sur \(I\).
\end{prop}

\begin{dem}
L'application \(\fonction{\Phi}{\Lendo{E}\times E}{E}{\paren{f,v}}{f.v}\) est bilinéaire.

\(E\) étant de dimension finie, \(\Lendo{E}\) aussi, donc \(\Phi\) est continue.

Donc si \(x\) est continue de \(I\) dans \(E\) et \(a\) est continue de \(I\) dans \(\Lendo{E}\), alors \(t\mapsto a\paren{t}.x\paren{t}\) est continue sur \(I\) par composition de fonctions continues.

Par conséquent, si \(x\) est solution de l'équation différentielle linéaire \(x\prim=a\paren{t}.x+b\paren{t}\), alors \(x\prim\) est continue sur \(I\), donc \(x\) est de classe \(\classe{1}\) sur \(I\).
\end{dem}

À partir de maintenant, le cours portera sur les équations différentielles, mais tout ce qui est énoncé à leur propos est valable pour les systèmes différentiels.

\subsection{Problème de Cauchy}

\begin{defi}
Soit \(\paren{*}:x\prim=a\paren{t}.x+b\paren{t}\) une équation différentielle linéaire qui respecte les conditions ci-dessus. On choisit un réel \(t_0\in I\) et un vecteur \(v_0\in E\).

Le problème d'existence et d'unicité d'une solution de \(\paren{*}\) qui satisfait la condition initiale \(x\paren{t_0}=v_0\) est appelé un problème de Cauchy.
\end{defi}

Le problème de Cauchy \[\paren{1}:\begin{dcases}
    x\prim=a\paren{t}.x+b\paren{t} \\
    x\paren{t_0}=v_0
\end{dcases}\] est équivalent au problème intégral \[\paren{2}:x\paren{t}=\int_{t_0}^t\paren{a\paren{\tau}.x\paren{\tau}+b\paren{\tau}}\odif{\tau}+v_0\] \ie \(x\) est solution de \(\paren{1}\) ssi \(x\) est solution de \(\paren{2}\).

\subsection{Équation différentielle linéaire homogène}

\begin{defi}
On appelle équation différentielle linéaire homogène (du premier ordre) toute équation différentielle de la forme \[x\prim=a\paren{t}.x\] où \(a\) est une fonction continue de \(I\) dans \(\Lendo{E}\).
\end{defi}

Il y a un seul résultat à connaître sur de telles équations : le théorème de Cauchy-Lipschitz (ou théorème de Cauchy linéaire).

\begin{theo}
L'ensemble des solutions d'une équation différentielle linéaire homogène \(x\prim=a\paren{t}.x\) est un sous-espace vectoriel de dimension \(n\) de \(\F{I}{\K}\).

Plus précisément, en notant \(S\) l'ensemble des solutions, pour tout \(t_0\in I\), l'application \[\fonctionlambda{S}{E}{x}{x\paren{t_0}}\] est un isomorphisme.

Autrement dit, pour tout \(t_0\in I\) et \(v_0\in E\) (appelé vecteur des conditions initiales), il existe une unique solution telle que \(x\paren{t_0}=v_0\).
\end{theo}

\begin{dem}[Cas particulier : \(I=\intervii{c}{b}\) est un compact]
On choisit \(\alpha>0\) à ajuster ultérieurement et on travaille sur \(\intervii{t_0}{t_0+\alpha}=J\).

On choisit une norme \(\norme{}\) sur \(E\) à laquelle on associe \(\normesub{}\) sur \(\Lendo{E}\).

\(a\) étant continue sur \(\intervii{c}{b}\), \(t\mapsto\normesub{a\paren{t}}\) est continue sur le segment \(\intervii{c}{b}\).

D'après le théorème des bornes atteintes, \(M=\sup_{t\in I}\normesub{a\paren{t}}\) existe dans \(\R\).

On pose \(x_0:t\mapsto v_0\) et pour tout \(k\in\N\) \[x_{k+1}:t\mapsto\int_{t_0}^ta\paren{\tau}.x_k\paren{\tau}\odif{\tau}+v_0.\]

On veut montrer que la suite de fonctions \(\paren{x_k}\) converge uniformément vers une fonction \(x\).

Par récurrence immédiate, les fonctions \(\paren{x_k}\) sont \(\classe{1}\) sur \(I\).

Pour \(k\in\Ns\), par linéarité de l'intégrale et de \(a\), on a \[x_{k+1}\paren{t}-x_k\paren{t}=\int_{t_0}^ta\paren{\tau}.\paren{x_k\paren{\tau}-x_{k-1}\paren{\tau}}\odif{\tau}.\]

Pour tout \(t\in J\), on a donc \[\begin{aligned}
\norme{x_{k+1}\paren{t}-x_k\paren{t}}&\leq\int_{t_0}^t\norme{a\paren{\tau}.\paren{x_k\paren{\tau}-x_{k-1}\paren{\tau}}}\odif{\tau} \\
&\leq\int_{t_0}^t\normesub{a\paren{\tau}}\norme{x_k\paren{\tau}-x_{k-1}\paren{\tau}}\odif{\tau} \\
&\leq M\int_{t_0}^t\norme{x_k\paren{\tau}-x_{k-1}\paren{\tau}}\odif{\tau} \\
&\leq M\norme{x_k-x_{k-1}}_\infty^J\paren{t-t_0} \\
&\leq M\alpha\norme{x_k-x_{k-1}}_\infty^J.
\end{aligned}\]

Donc \(\norme{x_{k+1}-x_k}_\infty^J\leq M\alpha\norme{x_k-x_{k-1}}_\infty^J\).

Maintenant, on choisit \(\alpha\) tel que \(M\alpha<1\).

Par récurrence immédiate, on en déduit que \[\quantifs{\forall k\in\N}\norme{x_{k+1}-x_k}_\infty^J\leq\underbrace{\paren{M\alpha}^k\norme{x_k-x_{k-1}}_\infty^J}_{\substack{\text{terme général d'une} \\ \text{série convergente}}}.\]

Donc la série de fonctions \(\sum_{k\geq0}\paren{x_{k+1}-x_k}\) converge normalement et donc uniformément sur \(J\) \ie la suite \(\paren{x_k}\) converge uniformément sur \(J\) vers la fonction \(x=\sum_{k=0}^{\pinf}\paren{x_{k+1}-x_k}+x_0\).

Pour tout \(k\in\N\) et \(t\in J\), on a \[x_{k+1}\paren{t}=\int_{t_0}^ta\paren{\tau}.x_k\paren{\tau}\odif{\tau}+v_0.\]

Or pour tout \(\tau\in J\), on a \[\begin{aligned}
\norme{a\paren{\tau}.\paren{x_k\paren{\tau}-x\paren{\tau}}}&\leq M\norme{x_k\paren{\tau}-x\paren{\tau}} \\
&\leq M\norme{x_k-x}_\infty^J \\
&\tendqd{k\to\pinf}0.
\end{aligned}\]

Donc \(\paren{t\mapsto a\paren{t}.x_k\paren{t}}_{k\in\N}\) converge uniformément vers \(t\mapsto a\paren{t}.x\paren{t}\) sur \(J\).

Donc par interversion limite-intégrale, on a \[\quantifs{\forall t\in J}x\paren{t}=\int_{t_0}^ta\paren{\tau}.x\paren{\tau}\odif{\tau}+v_0.\]

On remarque que ce procédé peut être réalisé sur tout intervalle de longueur \(\alpha\) et indépendamment de \(v_0\).

On réitère sur \(\intervii{t_0+\alpha}{t_0+2\alpha}\) avec la condition initiale \(x\paren{t_0+\alpha}=x\paren{t_0+\alpha}\), etc.

On peut recouvrir \(I\) par un nombre fini d'intervalles \(\intervii{t_0+j\alpha}{t_0+\paren{j+1}\alpha}\) où \(j\in\Z\).

En recollant les solutions locales, on obtient une solution globale sur \(I\) (et unique par unicité locale).
\end{dem}

\begin{dem}[Cas général]
On recolle sur \(I\) les solutions sur chaque segment inclus dans \(I\).

Cela justifie le théorème car la linéarité est triviale et car on a unicité donc isomorphisme.
\end{dem}

La conjonction d'une équation différentielle linéaire homogène et d'une condition initiale s'appelle un problème de Cauchy linéaire.

Ce qui précède signifie que n'importe quel problème de Cauchy linéaire possède une unique solution.

Une base de l'ensemble des solutions est appelé un système fondamental de solutions.

\begin{exo}
Montrez que les fonctions \(t\mapsto\dcoords{t\cos t}{t\sin t}\) et \(t\mapsto\dcoords{-t\sin t}{t\cos t}\) forment un système fondamental de solutions du système différentiel sur \(\Rps\) \(\begin{dcases}
    x\prim=\dfrac{1}{t}x-y \\
    y\prim=x+\dfrac{1}{t}y
\end{dcases}\)
\end{exo}

\begin{corr}~\\
On pose \(X=\dcoords{x}{y}\). Le système s'écrit \(X\prim\paren{t}=A\paren{t}X\paren{t}\) où \(A\paren{t}=\begin{pmatrix}
\nicefrac{1}{t} & -1 \\
1 & \nicefrac{1}{t}
\end{pmatrix}\).

D'après le théorème de Cauchy-Lipschitz, l'ensemble des solutions est un espace vectoriel de dimension \(2\).

Les fonctions \(t\mapsto\dcoords{t\cos t}{t\sin t}\) et \(t\mapsto\dcoords{-t\sin t}{t\cos t}\) sont solutions et non-colinéaires, donc elles forment une base \(\fami{S}\) de l'ensemble des solutions \ie un système fondamental de solutions.

On a \(\fami{S}=\accol{t\mapsto\dcoords{at\cos t-bt\sin t}{at\sin t+bt\cos t}\tq\paren{a,b}\in\R^2}\).
\end{corr}

\begin{exo}
Déterminez un système fondamental de solutions polynomiales du système \\ différentiel \(\begin{dcases}
    x\prim=\dfrac{t}{1+t^2}x+\dfrac{1}{1+t^2}y \\
    y\prim=\dfrac{-1}{1+t^2}x+\dfrac{t}{1+t^2}y
\end{dcases}\)
\end{exo}

\begin{corr}~\\
Si \(\dcoords{x}{y}\) est solution avec \(x=\lambda\) constante, la première équation donne \(y\paren{t}=-\lambda t\) et donc \[y\prim\paren{t}=-\lambda=\dfrac{-1}{1+t^2}\lambda+\dfrac{t}{1+t^2}\paren{-\lambda}t\] est vraie.

Donc \(t\mapsto\dcoords{-1}{t}\) est solution.

Idem, \(t\mapsto\dcoords{t}{1}\) est solution.

D'après le théorème de Cauchy-Lipschitz, \(\dim\fami{S}=2\) donc \[\fami{S}=\accol{t\mapsto\dcoords{-a+bt}{at+b}\tq\paren{a,b}\in\R^2}.\]
\end{corr}

\subsection{Cas général}

\begin{prop}
Soit \(x\prim=a\paren{t}.x+b\paren{t}\) une équation différentielle linéaire sur \(I\).

Si on connaît une solution \(p\) dite particulière de l'équation, alors on connaît la forme générale des solutions : elles sont toutes de la forme \[t\mapsto g\paren{t}+p\paren{t}\] où \(g\) est la solution générale de l'équation différentielle linéaire homogène associée.

Autrement dit, l'ensemble des solutions est un sous-espace affine de \(\F{I}{\K}\) : \(p+S\) où \(S\) est l'ensemble des solutions de l'équation homogène associée.
\end{prop}

Le théorème de Cauchy-Lipschitz est encore valable sous une forme amenuisée.

\begin{theo}
Pour tout \(t_0\in I\) et \(v_0\in E\), il existe une unique solution telle que \(x\paren{t_0}=v_0\).
\end{theo}

\begin{rem}
Il n'existe pas de méthode simple pour résoudre concrètement de telles équations différentielles linéaires, à part dans le cas où \(a\) est une constante.
\end{rem}

\section{Exponentielle d'un endomorphisme, d'une matrice}

\subsection{Définition}

\begin{prop}
Soit \(u\in\Lendo{E}\). La série \(\sum_{k\geq0}\dfrac{u^k}{k!}\) converge absolument dans \(\Lendo{E}\).

Soit \(A\in\M{n}\). La série \(\sum_{k\geq0}\dfrac{A^k}{k!}\) converge absolument dans \(\M{n}\).
\end{prop}

\begin{defi}
Avec les mêmes hypothèses, on pose \[\exp\paren{u}=\e{u}=\sum_{k=0}^{\pinf}\dfrac{u^k}{k!}\qquad\text{et}\qquad\exp\paren{A}=\e{A}=\sum_{k=0}^{\pinf}\dfrac{A^k}{k!}.\]
\end{defi}

Il est bien sûr évident que si \(u\) a pour matrice \(A\) dans une base \(\fami{B}\), alors \(\e{u}\) a pour matrice \(\e{A}\) dans cette même base.

\subsection{Propriétés algébriques}

L'exponentielle d'endomorphismes ou de matrices a de nombreuses propriétés communes avec l'exponentielle de nombres, mais pas toutes, car les algèbres \(\Lendo{E}\) et \(\M{n}\) ne sont pas commutatives.

\begin{prop}
\(\quantifs{\Tpt u\in\Lendo{E}}u\text{ et }\e{u}\) commutent. De même avec les matrices.
\end{prop}

\begin{prop}
Soient \(u,v\) deux endomorphismes de \(E\) qui commutent. On a \(\e{u+v}=\e{u}\e{v}=\e{v}\e{u}\).

De même avec des matrices de \(\M{n}\) qui commutent.
\end{prop}

Attention ! Sans hypothèse de commutation, ce résultat est faux en général.

Une conséquence immédiate est que \(\quantifs{\tpt u\in\Lendo{E}}\e{u}\) est inversible (donc non nul) et son inverse est \(\e{-u}\). De même pour les matrices.

Si \(A\) et \(B\) sont semblables, alors \(\e{A}\) et \(\e{B}\) le sont aussi avec la même matrice de passage : si \(B=PAP\inv\) alors \(\e{B}=P\e{A}P\inv\).

En général, il est difficile de calculer une exponentielle d'endomorphisme ou de matrice. Deux cas simples :

Dans le cas diagonalisable, c'est faisable, car l'exponentielle d'une matrice diagonale est très simple.

\begin{prop}
Soit \(A=\diag{\lambda_1,\dots,\lambda_n}\in\M{n}\).

On a \(\e{A}=\diag{\e{\lambda_1},\dots,\e{\lambda_n}}\).
\end{prop}

\begin{dem}
On a \(\quantifs{\forall k\in\N}A^k=\diag{\lambda_1^k,\dots,\lambda_n^k}\).

Donc \(\sum_{k=0}^N\dfrac{A^k}{k!}=\sum_{k=0}^N\dfrac{\diag{\lambda_1^k,\dots,\lambda_n^k}}{k!}=\begin{pmatrix}
\sum_{k=0}^N\dfrac{\lambda_1^k}{k!} & 0 & \dots & 0 \\
0 & \sum_{k=0}^N\dfrac{\lambda_2^k}{k!} & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \sum_{k=0}^N\dfrac{\lambda_n^k}{k!}
\end{pmatrix}\).

Par passage à la limite, on obtient \[\e{A}=\e{D}=\begin{pmatrix}
\e{\lambda_1} & 0 & \dots & 0 \\
0 & \e{\lambda_2} & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \e{\lambda_n}
\end{pmatrix}.\]
\end{dem}

Si une matrice \(A\) est nilpotente, alors son exponentielle est en fait un polynôme en \(A\).

Dans le cas général, le calcul est souvent pénible. Néanmoins, on a quelques résultats généraux simples.

\begin{prop}
Soit \(A\in\M{n}\) scindée.

On a \(\Sp{\e{A}}=\e{\Sp{A}}\).

En particulier, on a \(\det\e{A}=\e{\tr A}\).
\end{prop}

\subsection{Propriétés fonctionnelles}

La convergence absolue de la série définissant l'exponentielle entraîne quelques résultats simples à propos de la continuité et de la dérivabilité.

\begin{prop}
Les fonctions \(u\mapsto\e{u}\) et \(A\mapsto\e{A}\) sont continues sur \(\Lendo{E}\) et \(\M{n}\) respectivement.
\end{prop}

\begin{dem}[Continuité de \(u\mapsto\e{u}\)]
On choisit une norme \(\norme{}\) sous-multiplicative sur \(\Lendo{E}\).

Soit \(R>0\).

On a \(\quantifs{\forall u\in\boulef{0}{R};\forall k\in\N}\norme{u^k}\leq\norme{u}^k\leq R^k\).

Donc \(\quantifs{\forall k\in\N}\norme{\dfrac{u^k}{k!}}\leq\dfrac{R^k}{k!}\).

Donc la série \(\sum_{k=0}^{\pinf}\dfrac{u^k}{k!}\) converge normalement sur \(\boulef{0}{R}\).

Or les fonctions \(u\mapsto u^k\) sont continues.

Donc \(u\mapsto\e{u}=\sum_{k=0}^{\pinf}\dfrac{u^k}{k!}\) est continue sur \(\boulef{0}{R}\).

Donc par réunion des boules \(\boulef{0}{R}\), \(u\mapsto\e{u}\) est continue sur \(\Lendo{E}=\bigunion_{R>0}\boulef{0}{R}\).
\end{dem}

\begin{prop}
Soit \(u\in\Lendo{E}\).

La fonction de variable réelle \(t\mapsto\e{tu}\) est dérivable et sa dérivée est \(t\mapsto u\e{tu}=u\e{tu}\).
\end{prop}

\begin{dem}~\\
On a \(\e{tu}=\sum_{k=0}^{\pinf}t^k\dfrac{u^k}{k!}\).

On pose \(f_k:t\mapsto t^k\dfrac{u^k}{k!}\) de classe \(\classe{1}\) sur \(\R\) et, pour \(k\geq1\), on a \[\quantifs{\forall t\in\R}f_k\prim\paren{t}=kt^{k-1}\dfrac{u^k}{k!}=t^{k-1}\dfrac{u^k}{\paren{k-1}!}.\]

Soit \(a>0\). Pour \(t\in\intervii{-a}{a}\), on a \[\norme{f_k\prim\paren{t}}=\abs{t^{k-1}}\norme{\dfrac{u^k}{\paren{k-1}!}}\leq a^{k-1}\dfrac{\norme{u}^k}{\paren{k-1}!}.\]

Donc la série de fonctions \(\sum_{k=1}^{\pinf}f_k\prim\) converge normalement sur \(\intervii{-a}{a}\).

D'après le théorème de dérivation sous le signe somme, la fonction \(t\mapsto\e{tu}\) est de classe \(\classe{1}\) sur \(\intervii{-a}{a}\).

Par réunion d'intervalles, elle est de classe \(\classe{1}\) sur \(\R\) et \[\odv*{\paren{\e{tu}}}{t}=\paren{\sum_{k=1}^{\pinf}t^{k-1}\dfrac{u^{k-1}}{\paren{k-1}!}}\rond u=\e{tu}\rond u=u\e{tu}.\]
\end{dem}

\section{Équations linéaires homogènes à coefficients constants}

\subsection{Forme générale de la solution}

\begin{prop}
Soient \(a\in\Lendo{E}\), \(t_0\in I\) et \(v_0\in E\).

L'unique solution au problème de Cauchy \(\begin{dcases}
x\prim=a.x \\
x\paren{t_0}=v_0
\end{dcases}\) est la fonction \(t\mapsto\e{\paren{t-t_0}a}.v_0\).
\end{prop}

On a bien sûr l'équivalent matriciel.

\begin{prop}
Soient \(A\in\M{n}\), \(t_0\in I\) et \(V_0\in\K^n\).

L'unique solution au problème de Cauchy \(\begin{dcases}
X\prim=AX \\
X\paren{t_0}=V_0
\end{dcases}\) est la fonction \(t\mapsto\e{\paren{t-t_0}A}V_0\).
\end{prop}

\subsection{Cas praticable}

\begin{prop}
Soit \(A\in\M{n}\).

On suppose que \(A\) est diagonalisable : on note \(a_1,\dots,a_n\) les valeurs propres non-distinctes de \(A\) et \(\paren{V_1,\dots,V_n}\) une base de l'espace \(\M{n\,1}\) constituée de vecteurs propres associés.

Les solutions du système différentiel linéaire homogène \(X\prim=AX\) sont les fonctions \[t\mapsto\sum_{i=1}^n\lambda_i\e{a_it}V_i\] où \(\lambda_1,\dots,\lambda_n\) sont des constantes dites d'intégration.
\end{prop}

En pratique, on diagonalise la matrice \(A\) en donnant une matrice de passage \(P\) et une matrice diagonale \(D\) telles que \(A=PDP\inv\), on pose \(X=PY\) et on résout le système différentiel \(Y\prim=DY\), ce qui donne la forme générale de \(Y\) puis celle de \(X=PY\). Ce qui est remarquable, c'est qu'il est inutile de calculer \(P\inv\).

Quand la matrice \(A\) n'est pas diagonalisable mais seulement trigonalisable, alors en notant \(T\) une matrice triangulaire semblable à \(A\), la même stratégie peut être appliquée, moyennant une résolution un peu plus difficile du système différentiel \(Y\prim=TY\).

\begin{exo}
Résoudre le système différentiel \(\begin{dcases}
x\prim=2x+y-z \\
y\prim=x+2y-z \\
z\prim=-x-y+2z
\end{dcases}\)
\end{exo}

\begin{corr}~\\
On a \(A=\begin{pmatrix}
2 & 1 & -1 \\
1 & 2 & -1 \\
-1 & -1 & 2
\end{pmatrix}\in\sym{3}[\R]\) donc diagonalisable.

On a \(A-I_3=\begin{pmatrix}
1 & 1 & -1 \\
1 & 1 & -1 \\
-1 & -1 & 1
\end{pmatrix}\) de rang \(1\).

Donc \(1\) est valeur propre et \(\dim\sep{A}{1}=3-1=2\).

\(A\) est diagonalisable donc elle a trois valeurs propres : \(1\), \(1\) et \(\lambda\).

Or \(\tr A=6\) donc \(1+1+\lambda=6\) \ie \(\lambda=4\).

Avec \(U=\tcoords{a}{b}{c}\), on a \(AU=U\ssi a+b-c=0\).

Donc \(\sep{A}{1}=\Vect{V_1,V_2}\) où \(V_1=\tcoords{1}{0}{1}\) et \(V_2=\tcoords{0}{1}{1}\).

D'après le théorème spectral, on a \[\sep{A}{4}=\sep{A}{1}\ortho=\Vect{V_1\vecto V_2}=\Vect{V_3}\] où \(V_3=\tcoords{-1}{-1}{1}\).

On pose \(P=\begin{pmatrix}
1 & 0 & -1 \\
0 & 1 & -1 \\
1 & 1 & 1
\end{pmatrix}\) et on a alors \(A=PDP\inv\) où \(D=\diag{1,1,4}\).

On a \[\begin{aligned}
X\prim=AX&\ssi X\prim=PDP\inv X \\
&\ssi P\inv X\prim=DP\inv X.
\end{aligned}\]

En posant \(Y=P\inv X\), on a l'équivalence \(X\prim=AX\ssi Y\prim=DY\).

Avec \(Y=\tcoords{u}{v}{w}\), on a \[Y\prim=DY\ssi\begin{dcases}
u\prim=u \\
v\prim=v \\
w\prim=4w
\end{dcases}\]

Donc \(X\) est solution ssi il existe \(\paren{\alpha,\beta,\gamma}\in\R^3\) tels que \[\begin{dcases}
u\paren{t}=\alpha\e{t} \\
v\paren{t}=\beta\e{t} \\
w\paren{t}=\gamma\e{4t}
\end{dcases}\] \ie \[X=PY=\tcoords{\alpha\e{t}-\gamma\e{4t}}{\beta\e{t}-\gamma\e{4t}}{\paren{\alpha+\beta}\e{t}+\gamma\e{4t}}.\]
\end{corr}

\begin{exo}
Résoudre le système différentiel \(\begin{dcases}
x\prim=-2x-y+7z \\
y\prim=5x+4y-8z \\
z\prim=x+y+z
\end{dcases}\)
\end{exo}

\begin{corr}~\\
On pose \(A=\begin{pmatrix}
-2 & -1 & 7 \\
5 & 4 & -8 \\
1 & 1 & 1
\end{pmatrix}\in\M{3}[\R]\).

On a \[\chi_A=\begin{vmatrix}
X+2 & 1 & -7 \\
-5 & X-4 & 8 \\
-1 & -1 & X-1
\end{vmatrix}=\paren{X+1}\paren{X-2}^2.\]

De plus, on a \[\begin{aligned}
AU=U&\ssi\begin{dcases}
-x-y+7z=0 \\
5x+5y-8z=0 \\
x+y+2z=0
\end{dcases} \\
&\ssi\begin{dcases}
9z=0 \\
-6z=0 \\
x+y=0
\end{dcases} \\
&\ssi U\in\Vect{V_1}\text{ où }V_1=\tcoords{-1}{1}{0}.
\end{aligned}\]

Idem, \(\sep{A}{2}=\Vect{V_2}\) où \(V_2=\tcoords{2}{-1}{1}\).

\(\dim\sep{A}{2}=1\not=2\) donc \(A\) n'est pas diagonalisable.

Or \(A\) est scindée donc trigonalisable : il existe \(P\in\GL{3}[\R]\) et \(T\in\Tsup{3}[\R]\) telles que \(A=PTP\inv\).

On cherche \(T\) sous la forme \(T=\begin{pmatrix}
-1 & 0 & 0 \\
0 & 2 & 1 \\
0 & 0 & 2
\end{pmatrix}\) \ie on cherche \(V\not\in\Vect{V_1,V_2}\) tel que \(AV=2V+V_2\).

Avec \(V=\tcoords{a}{b}{c}\), on a \[\begin{aligned}
AV=2V+V_2&\ssi\begin{dcases}
-2a-b+7c=2a+2 \\
5a+4b-8c=2b-1 \\
a+b+c=2c+1
\end{dcases} \\
&\ssi\begin{dcases}
a+b-c=1 \\
-4a-b+7c=2 \\
5a+2b-8c=-1
\end{dcases} \\
&\ssi\begin{dcases}
a+b-c=1 \\
3b+3c=6 \\
-3b-3c=-6
\end{dcases} \\
&\ssi\begin{dcases}
a+b+c=1 \\
b+c=2
\end{dcases} \\
&\ssi\begin{dcases}
a=-1+2c \\
b=2-c
\end{dcases}
\end{aligned}\]

Par exemple, \(V=\tcoords{-1}{2}{0}\) convient.

On pose \(P=\begin{pmatrix}
-1 & 2 & -1 \\
1 & -1 & 2 \\
0 & 1 & 0
\end{pmatrix}\).

On a \[\begin{aligned}
X\prim=AX&\ssi P\inv X=TP\inv X \\
&\ssi Y\prim=TY
\end{aligned}\] où \(Y=P\inv X=\tcoords{u}{v}{w}\).

On a \(\begin{dcases}
u\prim=-u \\
v\prim=2v+w \\
w\prim=2w
\end{dcases}\)

Or \begin{itemize}
    \item \(u\prim=-u\ssi u\paren{t}=\alpha\e{-t}\) ;
    \item \(w\prim=2w\ssi w\paren{t}=\gamma\e{2t}\) ;
    \item \(v\prim=2v+\gamma\e{2t}\ssi v\prim-2v=\gamma\e{2t}\ssi v\paren{t}=\beta\e{2t}+\gamma t\e{2t}\). \\
\end{itemize}

Donc \[X=PY=\tcoords{-\alpha\e{-t}+2\e{2t}\paren{\gamma t+\beta}-\gamma\e{2t}}{\alpha\e{-t}-\e{2t}\paren{\gamma t+\beta}+2\gamma\e{2t}}{\e{2t}\paren{\gamma t+\beta}}=\alpha\tcoords{-\e{-t}}{\e{-t}}{0}+\beta\tcoords{2\e{2t}}{-\e{2t}}{\e{2t}}+\gamma\tcoords{\paren{2t-1}\e{2t}}{\paren{2-t}\e{2t}}{t\e{2t}}.\]
\end{corr}

Dans la suite, on s'intéresse à d'autres types d'équations différentielles linéaires, où les inconnues sont à valeurs réelles ou complexes.

\section{Équations différentielles linéaires scalaires d'ordre \(n\)}

\subsection{Généralités}

\begin{defi}
On appelle équation différentielle linéaire scalaire d'ordre \(n\) toute équation différentielle de la forme \[y\deriv{n}=a_{n-1}\paren{t}y\deriv{n-1}+\dots+a_1\paren{t}y\prim+a_0\paren{t}y+b\paren{t}\] où \(a_0,\dots,a_{n-1},b\) sont \(n+1\) fonctions continues de \(I\) dans \(\K\) et \(y\) est la fonction inconnue de \(I\) dans \(\K\).
\end{defi}

\begin{defi}
Avec les notations précédentes, on appelle solution (sur \(I\)) de l'équation différentielle toute application \(f\) \(n\) fois dérivable sur \(I\) et telle que \[\quantifs{\forall t\in I}f\deriv{n}\paren{t}=a_{n-1}\paren{t}f\deriv{n-1}\paren{t}+\dots+a_1\paren{t}f\prim\paren{t}+a_0\paren{t}f\paren{t}+b\paren{t}.\]
\end{defi}

\begin{ex}
\begin{itemize}
    \item \(y\seconde-\paren{t-1}y\prim+ty=0\) a pour solution sur \(\R\) la fonction \(\exp\). \\
    \item \(y\seconde+\dfrac{1}{t}y\prim+4t^2y=1+t^4\) a pour solution sur \(\Rs\) la fonction \(t\mapsto\cos\paren{t^2}+\dfrac{t^2}{4}\).
\end{itemize}
\end{ex}

\begin{prop}
Les solutions d'une telle équation différentielle linéaire sont au moins de classe \(\classe{n}\) sur \(I\).
\end{prop}

\subsection{Représentation matricielle}

On considère une équation différentielle linéaire scalaire d'ordre \(n\) \[\paren{1}:y\deriv{n}=a_{n-1}\paren{t}y\deriv{n-1}+\dots+a_1\paren{t}y\prim+a_0\paren{t}y+b\paren{t}\]

On pose \(X=\begin{pmatrix}
y \\
y\prim \\
\vdots \\
y\deriv{n-1}
\end{pmatrix}\), \(A=\begin{pmatrix}
0 & 1 & 0 & \dots & 0 \\
0 & 0 & 1 & \dots & 0 \\
\vdots & \vdots & \ddots & \ddots & \vdots \\
0 & 0 & \dots & 0 & 1 \\
a_0 & a_1 & \dots & a_{n-2} & a_{n-1}
\end{pmatrix}\) et \(B=\begin{pmatrix}
0 \\
\vdots \\
0 \\
b
\end{pmatrix}\).

On définit ainsi trois fonctions vectorielles (matricielles) \(X\in\ensclasse{1}{I}{\K^n}\), \(A\in\ensclasse{0}{I}{\M{n}}\) et \(B\in\ensclasse{0}{I}{\K^n}\).

L'équation \(\paren{1}\) est alors équivalente au système différentiel \(X\prim=AX+B\).

Par conséquent, tous les résultats précédents sont applicables dans ce cas particulier.

\begin{dem}
On a l'équivalence \[y\text{ solution de }(1)\ssi y\deriv{n}=\sum_{i=0}^{n-1}a_iy\deriv{i}+b.\]

On pose \(U=\tcoords{x_1}{\vdots}{x_n}\) où \(x_1,\dots,x_n\) sont \(n\) fonctions scalaires dérivables.

On a \[\begin{aligned}
U\prim=AU+B&\ssi\tcoords{x_1\prim}{\vdots}{x_n\prim}=\begin{pmatrix}
0 & 1 & 0 & \dots & 0 \\
0 & 0 & 1 & \dots & 0 \\
\vdots & \vdots & \ddots & \ddots & \vdots \\
0 & 0 & \dots & 0 & 1 \\
a_0 & a_1 & \dots & a_{n-2} & a_{n-1}
\end{pmatrix}\tcoords{x_1}{\vdots}{x_n}+\begin{pmatrix}
0 \\
\vdots \\
0 \\
b
\end{pmatrix} \\
&\ssi\begin{cases}
x_1\prim=x_2 \\
\vdots \\
x_{n-1}\prim=x_n \\
x_n\prim=\sum_{i=0}^{n-1}a_ix_{i+1}+b
\end{cases} \\
&\ssi\begin{cases}
x_1\prim=x_2 \\
\vdots \\
x_{n-1}\prim=x_n \\
x_n\prim=\sum_{i=0}^{n-1}a_ix_1\deriv{i}+b
\end{cases}
\end{aligned}\]

D'où \((1)\ssi X\prim=AX+B\).
\end{dem}

\subsection{Équation différentielle linéaire homogène}

\begin{defi}
On appelle équation différentielle linéaire scalaire homogène d'ordre \(n\) toute équation différentielle de la forme \[y\deriv{n}=a_{n-1}\paren{t}y\deriv{n-1}+\dots+a_1\paren{t}y\prim+a_0\paren{t}y\] où \(a_0,\dots,a_{n-1}\) sont \(n\) fonctions continues sur \(I\) à valeurs dans \(\K\) et \(y\) est la fonction inconnue de \(I\) dans \(\K\).
\end{defi}

Là encore, il y a un seul résultat à connaître sur de telles équations : le théorème de Cauchy-Lipschitz.

\begin{theo}
L'ensemble des solutions d'une équation différentielle linéaire scalaire homogène d'ordre \(n\) est un \(\K\)-espace vectoriel de dimension \(n\).

Plus précisément, en notant \(S\) l'ensemble des solutions, pour tout \(t_0\in I\), l'application \[\fonctionlambda{S}{\K^n}{f}{\paren{f\paren{t_0},f\prim\paren{t_0},\dots,f\deriv{n-1}\paren{t_0}}}\] est un isomorphisme.

Autrement dit, pour tout \(t_0\in I\) et \(\paren{v_0,v_1,\dots,v_{n-1}}\in\K^n\) (appelé vecteur des conditions initiales), il existe une unique solution telle que \(\quantifs{\tpt k\in\interventierii{0}{n-1}}f\deriv{k}\paren{t_0}=v_k\).
\end{theo}

Là encore, toute base de l'ensemble des solutions est appelée système fondamental de solutions. Il n'y a hélas pas de méthode générale pour en calculer concrètement.

\subsection{Cas général}

\begin{prop}
Soit \[\paren{1}:y\deriv{n}=a_{n-1}\paren{t}y\deriv{n-1}+\dots+a_1\paren{t}y\prim+a_0\paren{t}y+b\paren{t}\] une équation différentielle linéaire scalaire d'ordre \(n\) sur l'intervalle \(I\).

Si on connaît une solution \(p\) dite particulière de l'équation, alors on connaît la forme générale des solutions : elles sont de la forme \[t\mapsto g\paren{t}+p\paren{t}\] où \(g\) est la solution générale de l'équation différentielle linéaire homogène associée.
\end{prop}

Le théorème de Cauchy-Lipschitz est encore valable sous une forme amenuisée.

\begin{theo}
Pour tout choix des conditions initiales \(\paren{v_0,\dots,v_{n-1}}\in\K^n\), le problème de Cauchy \[\begin{dcases}
y\deriv{n}=a_{n-1}\paren{t}y\deriv{n-1}+\dots+a_1\paren{t}y\prim+a_0\paren{t}y+b\paren{t} \\
y\paren{t_0}=v_0 \\
y\prim\paren{t_0}=v_1 \\
\vdots \\
y\deriv{n-1}\paren{t_0}=v_{n-1}
\end{dcases}\] possède toujours une unique solution.
\end{theo}

\begin{rem}
La recherche d'une solution particulière est un problème difficile. Sauf cas évidents, des indications seront données.
\end{rem}

\subsection{Diverses idées pour résoudre une équation différentielle linéaire d'ordre \(n\)}

En pratique, on a souvent \(n=2\).

\begin{itemize}
    \item Chercher des solutions sous une forme \textit{a priori} raisonnable (polynomiale, exponentielle, etc.). \\
    \item Chercher des solutions développables en série entière. \\
    \item Faire un changement de variable ou un changement de fonction inconnue en suivant les indications de l'énoncé. \\
    \item Quand on connaît une solution non-nulle \(z\), en chercher une autre sous la forme \(uz\) où \(u\) est une fonction inconnue à déterminer (c'est la méthode de variation de la constante !), qui transforme le problème différentiel d'ordre \(n\) en un problème d'ordre \(n-1\) plus une intégration.
\end{itemize}

\begin{exo}
Résolvez l'équation différentielle suivante sur \(\intervee{-\dfrac{1}{2}}{\pinf}\) : \(\paren{2t+1}y\seconde\paren{t}+\paren{4t-2}y\prim\paren{t}-8y\paren{t}=0\) en cherchant une solution polynomiale et une solution exponentielle.
\end{exo}

\begin{corr}
Si \(f\) est une solution polynomiale de degré \(n\), on a \[f\paren{t}=t^n+\dots\qquad f\prim\paren{t}=nt^{n-1}+\dots\qquad f\seconde\paren{t}=n\paren{n-1}t^{n-2}+\dots\]

Alors \(\paren{2t+1}f\seconde\paren{t}+\paren{4t-2}f\prim\paren{t}-8f\paren{t}=0\) donne \[\paren{2n\paren{n-1}t^{n-1}+\dots}+\paren{4nt^n+\dots}+\paren{-8t^n+\dots}=0.\]

Donc \(n=2\) \ie \[f\paren{t}=t^2+at+b\qquad f\prim\paren{t}=2t+a\qquad f\seconde\paren{t}=2.\]

Donc \[\begin{aligned}
\paren{2t+1}2+\paren{4t-2}\paren{2t+a}-8\paren{t^2+at+b}=0&\ssi-4at+2-2a-8b=0 \\
&\ssi\begin{dcases}
-4a=0 \\
2-2a-8b=0
\end{dcases} \\
&\ssi\begin{dcases}
a=0 \\
b=\dfrac{1}{4}
\end{dcases}
\end{aligned}\]

Donc \(f:t\mapsto t^2+\dfrac{1}{4}\) est solution.

De plus, si \(f:t\mapsto\e{kt}\) est solution, alors \[\begin{aligned}
\paren{2t+1}k^2\e{kt}+\paren{4t-2}k\e{kt}-8\e{kt}=0&\ssi k^2\paren{2t+1}+k\paren{4t-2}-8=0 \\
&\ssi\begin{dcases}
2k^2+4k=0 \\
k^2-2k-8=0
\end{dcases} \\
&\ssi k=-2.
\end{aligned}\]

Donc la famille \(\paren{t\mapsto t^2+\dfrac{1}{4},t\mapsto\e{-2t}}\) est une famille libre de solutions.

Or d'après le théorème de Cauchy-Lipschitz, l'ensemble des solutions est un \(\R\)-espace vectoriel de dimension \(2\) \ie \[\fami{S}=\accol{t\mapsto\lambda\paren{t^2+\dfrac{1}{4}}+\mu\e{-2t}\tq\paren{\lambda,\mu}\in\R^2}.\]
\end{corr}

\begin{exo}
Même exercice sur \(\Rps\) avec : \(y\seconde\paren{t}+2t\paren{1+t}y\prim\paren{t}-2\paren{1+t}y\paren{t}=0\) en cherchant une première solution de la forme \(t\mapsto t^\alpha\) où \(\alpha\in\R\).
\end{exo}

\begin{corr}
\note{exercice trop compliqué, correction abandonnée en cours}
\end{corr}

\begin{exo}
Même exercice sur \(\R\) avec : \(\paren{1+t^2}y\seconde\paren{t}+4ty\prim\paren{t}+2y\paren{t}=1\) en cherchant des solutions développables en série entière.
\end{exo}

\begin{corr}~\\
Avec \(f:t\mapsto\sum_{n=0}^{\pinf}a_nt^n\) on a \[f\prim\paren{t}=\sum_{n=0}^{\pinf}na_nt^{n-1}\qquad\text{et}\qquad f\seconde\paren{t}=\sum_{n=0}^{\pinf}n\paren{n-1}a_nt^{n-2}.\]

Alors \[\begin{aligned}
f\text{ solution}&\ssi\sum_{n=2}^{\pinf}n\paren{n-1}a_nt^{n-2}+\sum_{n=0}^{\pinf}n\paren{n-1}a_nt^n+\sum_{n=0}^{\pinf}4na_nt^n+\sum_{n=0}^{\pinf}2a_nt^n=1 \\
&\ssi\sum_{n=0}^{\pinf}\paren{n+2}\paren{n+1}a_{n+2}t^n+\sum_{n=0}^{\pinf}\paren{n\paren{n-1}+4n+2}a_nt^n=1 \\
&\ssi\begin{dcases}
2a_2+2a_0=1 \\
\quantifs{\forall n\geq1}\paren{n+2}\paren{n+1}a_{n+2}+\paren{n^2+3n+2}a_n=0
\end{dcases} \\
&\ssi\begin{dcases}
a_2=\dfrac{1}{2}-a_0 \\
\quantifs{\forall n\geq1}a_{n+2}=-a_n
\end{dcases} \\
&\ssi\begin{dcases}
a_0\text{ quelconque} \\
a_1\text{ quelconque} \\
a_2=\dfrac{1}{2}-a_0 \\
\quantifs{\forall n\in\Ns}\begin{dcases}
a_{2n}=\paren{-1}^{n-1}a_2 \\
a_{2n+1}=\paren{-1}^na_n
\end{dcases}
\end{dcases}
\end{aligned}\]

Donc, si \(f\) est développable en série entière au voisinage de \(0\), alors \[f\paren{t}=a_1\sum_{n=0}^{\pinf}\paren{-1}^nt^{2n+1}+a_0+\paren{\dfrac{1}{2}-a_0}\sum_{n=1}^{\pinf}\paren{-1}^{n-1}t^{2n}.\]

Pour \(t\in\intervee{-1}{1}\), on pose \[\phi\paren{t}=\sum_{n=0}^{\pinf}\paren{-1}^nt^{2n+1}\qquad\text{et}\qquad\psi\paren{t}=\sum_{n=1}^{\pinf}\paren{-1}^{n-1}t^{2n}.\]

Alors si \(f\) est une solution développable en série entière, on a \[f\paren{t}=a_1\phi\paren{t}+\paren{\dfrac{1}{2}-a_0}\paren{\psi\paren{t}-1}+\dfrac{1}{2}.\]

Réciproquement, si \(f\) est de cette forme, alors \(f\) est développable en série entière sur \(\intervee{-1}{1}\) et est solution de l'équation différentielle.

Or pour \(t\in\intervee{-1}{1}\), on a \[\phi\paren{t}=t\sum_{n=0}^{\pinf}\paren{-t^2}^n=\dfrac{t}{1+t^2}\] et \[\psi\paren{t}=-\sum_{n=1}^{\pinf}\paren{-t^2}^n=\dfrac{t^2}{1+t^2}.\]

On vérifie que \(t\mapsto\dfrac{t}{1+t^2}\) et \(t\mapsto\dfrac{t^2}{1+t^2}-1\) sont solutions sur \(\R\) et elles sont non-colinéaires donc d'après le théorème de Cauchy-Lipschitz : \[\fami{S}=\accol{t\mapsto\lambda\dfrac{t}{1+t^2}+\mu\paren{\dfrac{t^2}{1+t^2}-1}+\dfrac{1}{2}\tq\paren{\lambda,\mu}\in\R^2}.\]
\end{corr}

\begin{exo}
Même exercice sur \(\Rps\) avec : \(t^2y\seconde\paren{t}+4ty\prim\paren{t}-\paren{t^2-2}y\paren{t}=\e{t}\) en effectuant le changement de fonction inconnue \(z\paren{t}=t^2y\paren{t}\).
\end{exo}

\begin{corr}
Avec \(z:t\mapsto t^2y\paren{t}\), on a \[z\prim\paren{t}=2ty\paren{t}+t^2y\prim\paren{t}\] et \[z\seconde\paren{t}=t^2y\seconde\paren{t}+4ty\prim\paren{t}+2y\paren{t}.\]

On a donc \[t^2y\seconde+4ty\prim-\paren{t^2-2}y=z\seconde-z=\e{t}.\]

Donc \(z\) est de la forme \(t\mapsto\alpha\e{t}+\beta\e{-t}+\dfrac{1}{2}t\e{t}\) avec \(\paren{\alpha,\beta}\in\R^2\).

Sur \(\Rps\), on a donc \[\fami{S}=\accol{t\mapsto\alpha\dfrac{\e{t}}{t^2}+\beta\dfrac{\e{-t}}{t^2}+\dfrac{\e{t}}{2t}\tq\paren{\alpha,\beta}\in\R^2}.\]
\end{corr}

Dans le cas \(n=2\), pour trouver une solution particulière de l'équation \(y\seconde=a\paren{t}y\prim+b\paren{t}y+c\paren{t}\), si on connaît un système fondamental de solutions de l'équation homogène associée \(y\seconde=a\paren{t}y\prim+b\paren{t}y\), alors on peut parfois réussir à trouver une solution particulière par variations des constantes : en notant \(\paren{s_1,s_2}\) un système fondamental de solutions de l'équation homogène associée, on pose \(p=u_1s_1+u_2s_2\) où \(u_1\) et \(u_2\) sont deux fonctions inconnues deux fois dérivables. Comme on a deux inconnues pour une seule équation, on impose une condition supplémentaire : \(u_1\prim s_1+u_2\prim s_2=0\), afin que la recherche de solutions particulières aboutisse au système suivant : \[\begin{dcases}
u_1\prim s_1+u_2\prim s_2=0 \\
u_1\prim s_1\prim+u_2\prim s_2\prim=c
\end{dcases}\] Comme dans le cas de la méthode de variation de la constante vue en première année, la détermination explicite de \(u_1\) et \(u_2\) est parfois empêchée par les calculs de primitives non-explicitables.

\begin{dem}
Avec \(p=u_1s_1+u_2s_2\), on a \[p\prim=u_1\prim s_1+u_2\prim s_2+u_1s_1\prim+u_2s_2\prim\] et \[p\seconde=u_1\seconde s_1+2u_1\prim s_1\prim+u_1s_1\seconde+u_2\seconde s_2+2u_2\prim s_2\prim+u_2s_2\seconde.\]

Alors \[\begin{aligned}
p\text{ solution}&\ssi p\seconde=ap\prim+bp+c \\
&\ssi u_1\seconde s_1+2u_1\prim s_1\prim+\textcolor{green}{\bcancel{u_1s_1\seconde}}+u_2\seconde s_2+2u_2\prim s_2\prim+\textcolor{blue}{\bcancel{u_2s_2\seconde}}=au_1\prim s_1+au_2\prim s_2+\textcolor{green}{\bcancel{au_1s_1\prim}}+\textcolor{blue}{\bcancel{au_2s_2\prim}} \\
&\phantom{\ssi u_1\seconde s_1+2u_1\prim s_1\prim+u_1s_1\seconde+u_2\seconde s_2+2u_2\prim s_2\prim+u_2s_2\seconde=+}+\textcolor{green}{\bcancel{bu_1s_1}}+\textcolor{blue}{\bcancel{bu_2s_2}}+c \\
&\ssi\paren{u_1\prim s_1+u_2\prim s_2}\prim+u_1\prim s_1\prim+u_2\prim s_2\prim=\underbrace{au_1\prim s_1+au_2\prim s_2}_{=a\paren{u_1\prim s_1+u_2\prim s_2}}+c.
\end{aligned}\]

En effet, on a \[u_1\seconde s_1+2u_1\prim s_1\prim=u_1\seconde s_1+u_1\prim s_1\prim+u_1\prim s_1\prim=\paren{u_1\prim+s_1}\prim+u_1\prim s_1\prim\] et on justifie les annulations par \(s_1\seconde=as_1\prim+bs_1\) et \(s_2\seconde=as_2\prim+bs_2\).

Si on choisit \(u_1\prim s_1+u_2\prim s_2=0\) alors \[p\text{ solution}\ssi u_1\prim s_1\prim+u_2\prim s_2\prim=c.\]
\end{dem}

\begin{exo}
Résolvez l'équation \(y\seconde+3y\prim+2y=\dfrac{t-1}{t^2}\e{-t}\) sur \(\intervee{0}{\pinf}\).
\end{exo}

\begin{corr}
Les solutions homogènes sont \(t\mapsto\lambda\e{-t}+\mu\e{-2t}\) avec \(\paren{\lambda,\mu}\in\R^2\).

On pose \[p\paren{t}=u_1\paren{t}\e{-t}+u_2\paren{t}\e{-2t}\] et on impose \[u_1\prim\paren{t}\e{-t}+u_2\prim\paren{t}\e{-2t}=0.\]

Alors \[\begin{aligned}
p\text{ solution}&\ssi-u_1\prim\paren{t}\e{-t}-2u_2\prim\paren{t}\e{-2t}=\dfrac{t-1}{t^2}\e{-t} \\
&\ssi\begin{dcases}
u_1\prim\paren{t}+\e{-t}u_2\prim\paren{t}=0 \\
-u_1\prim\paren{t}-2\e{-t}u_2\prim\paren{t}=\dfrac{t-1}{t^2}
\end{dcases} \\
&\ssi\begin{dcases}
u_1\prim\paren{t}=\dfrac{t-1}{t^2} \\
u_2\prim\paren{t}=\dfrac{1-t}{t^2}\e{t}
\end{dcases}
\end{aligned}\]

On choisit par exemple \[u_1\paren{t}=\ln t+\dfrac{1}{t}\] et \[u_2\paren{t}=-\dfrac{\e{t}}{t}.\]

Conclusion : \[\fami{S}=\accol{t\mapsto\lambda\e{-t}+\mu\e{-2t}+\ln\paren{t}\e{-t}\tq\paren{\lambda,\mu}\in\R^2}.\]
\end{corr}

\begin{exo}
Résolvez l'équation \(y\seconde+4y=\tan t\) sur \(\intervee{-\dfrac{\pi}{2}}{\dfrac{\pi}{2}}\).
\end{exo}

\begin{corr}
Les solutions homogènes sont \(t\mapsto\lambda\cos\paren{2t}+\mu\sin\paren{2t}\) avec \(\paren{\lambda,\mu}\in\R^2\).

On pose \[p\paren{t}=u_1\paren{t}\cos\paren{2t}+u_2\paren{t}\sin\paren{2t}\] et on impose \[u_1\prim\paren{t}\cos\paren{2t}+u_2\prim\paren{t}\sin\paren{2t}=0.\]

Alors \[p\text{ solution}\ssi\begin{dcases}
u_1\prim\paren{t}\cos\paren{2t}+u_2\prim\paren{t}\sin\paren{2t}=0 \\
-2u_1\prim\paren{t}\sin\paren{2t}+2u_2\prim\paren{t}\cos\paren{2t}=\tan t
\end{dcases}\]

Or \(\begin{vmatrix}
\cos\paren{2t} & \sin\paren{2t} \\
-2\sin\paren{2t} & 2\cos\paren{2t}
\end{vmatrix}=2\) donc \[\begin{dcases}
u_1\prim\paren{t}=\dfrac{\begin{vmatrix}0 & \sin\paren{2t} \\ \tan t & 2\cos\paren{2t}\end{vmatrix}}{2}=\dfrac{-\tan\paren{t}\sin\paren{2t}}{2}=-\sin^2t=\dfrac{\cos\paren{2t}-1}{2} \\
u_2\prim\paren{t}=\dfrac{\begin{vmatrix}\cos\paren{2t} & 0 \\ -2\sin\paren{2t} & \tan t\end{vmatrix}}{2}=\dfrac{\cos\paren{2t}\tan\paren{t}}{2}=\dfrac{\paren{2\cos^2t-1}\frac{\sin t}{\cos t}}{2}=\dfrac{\sin\paren{2t}}{2}-\dfrac{1}{2}\dfrac{\sin t}{\cos t}
\end{dcases}\]

On choisit par exemple \[u_1\paren{t}=\dfrac{1}{4}\sin\paren{2t}-\dfrac{t}{2}\] et \[u_2\paren{t}=-\dfrac{1}{4}\cos\paren{2t}+\dfrac{1}{2}\ln\abs{\cos t}.\]

Conclusion : \[\fami{S}=\accol{t\mapsto\lambda\cos\paren{2t}+\mu\sin\paren{2t}-\dfrac{t}{2}\cos\paren{2t}+\dfrac{1}{2}\sin\paren{2t}\ln\abs{\cos t}\tq\paren{\lambda,\mu}\in\R^2}.\]
\end{corr}
